{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TMoW8ofJc-y3",
        "Y3UmrwGWcMoM",
        "-OiICdWEci3F",
        "dFTfIG0AfFaK"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Installation and environment variable**\n"
      ],
      "metadata": {
        "id": "TMoW8ofJc-y3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vjkZj2MEDGh",
        "outputId": "679212b3-0136-4147-c74c-375dcc978a68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qdrant-client in /usr/local/lib/python3.11/dist-packages (1.13.3)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
            "Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (1.71.0)\n",
            "Requirement already satisfied: grpcio-tools>=1.41.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (1.71.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.0.2)\n",
            "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.10.1)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.11.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.3.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.164.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from grpcio-tools>=1.41.0->qdrant-client) (75.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.14.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.8->qdrant-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.8->qdrant-client) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.8->qdrant-client) (0.4.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m172.0/211.5 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\n",
            "    unknown package:\n",
            "        Expected sha256 f083fc24912aa410be21fa16d157fed2055dab1cc4b6934a0e03cba69eb242b9\n",
            "             Got        c68eb8480823a6d130d096a1ab17441a5279924658a906272c488cb9b6103d8d\n",
            "\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install qdrant-client google-generativeai rank-bm25 sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QDRANT_HOST = \"https://fd98050b-4928-44e1-9536-66478313e9c5.us-west-1-0.aws.cloud.qdrant.io\"\n",
        "\n",
        "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.EHI0nboatGy3QWLI6mf5tgpoGaLhSeNR9TtoetM16bE\"\n",
        "\n",
        "GEMINI_API_KEY = \"AIzaSyBLWAaYmdhyTHlXAnULNGVxuxx21sq_HBg\""
      ],
      "metadata": {
        "id": "fa7tOl2VF_Oj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dense Retrieval RAG**"
      ],
      "metadata": {
        "id": "Y3UmrwGWcMoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
        "\n",
        "# ENV variables\n",
        "QDRANT_HOST = \"https://fd98050b-4928-44e1-9536-66478313e9c5.us-west-1-0.aws.cloud.qdrant.io\"\n",
        "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.EHI0nboatGy3QWLI6mf5tgpoGaLhSeNR9TtoetM16bE\"\n",
        "GEMINI_API_KEY = \"AIzaSyBLWAaYmdhyTHlXAnULNGVxuxx21sq_HBg\"\n",
        "\n",
        "# Initialize Gemini\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Step 1: Initialize Qdrant client\n",
        "qdrant = QdrantClient(\n",
        "    url=QDRANT_HOST,\n",
        "    api_key=QDRANT_API_KEY,\n",
        ")\n",
        "\n",
        "COLLECTION_NAME = \"Dense-Retrieval\"\n",
        "\n",
        "# Step 2: Create collection (if not exists)\n",
        "try:\n",
        "    qdrant.get_collection(collection_name=COLLECTION_NAME)\n",
        "    print(f\"âœ… Collection '{COLLECTION_NAME}' already exists.\")\n",
        "except Exception:\n",
        "    qdrant.create_collection(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
        "    )\n",
        "    print(f\"âœ… Collection '{COLLECTION_NAME}' created.\")\n",
        "\n",
        "# Step 3: Sample documents\n",
        "docs = [\n",
        "    {\"id\": \"1\", \"text\": \"We are experiencing delays in shipping due to weather conditions.\"},\n",
        "    {\"id\": \"2\", \"text\": \"Shipping may take 5-7 business days during holiday seasons.\"},\n",
        "    {\"id\": \"3\", \"text\": \"Refunds are processed within 3-5 business days.\"},\n",
        "]\n",
        "\n",
        "# Step 4: Embed documents & upload to Qdrant\n",
        "points = []\n",
        "for doc in docs:\n",
        "    response = genai.embed_content(model=\"models/embedding-001\", content=doc[\"text\"])\n",
        "    embedding = response[\"embedding\"]\n",
        "    points.append(PointStruct(id=int(doc[\"id\"]), vector=embedding, payload={\"text\": doc[\"text\"]}))\n",
        "\n",
        "qdrant.upsert(collection_name=COLLECTION_NAME, points=points)\n",
        "print(\"âœ… Documents upserted to Qdrant.\")\n",
        "\n",
        "# Step 5: Query\n",
        "query = \"Why is my order late?\"\n",
        "\n",
        "# Step 6: Embed query\n",
        "query_response = genai.embed_content(model=\"models/embedding-001\", content=query)\n",
        "query_vector = query_response[\"embedding\"]\n",
        "\n",
        "# Step 7: Search Qdrant\n",
        "hits = qdrant.search(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    query_vector=query_vector,\n",
        "    limit=3,\n",
        ")\n",
        "\n",
        "# Step 8: Display Results\n",
        "print(\"\\nğŸ” Top results for query:\", query)\n",
        "for hit in hits:\n",
        "    print(f\"- {hit.payload['text']} (Score: {hit.score:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "M2WJktVtEfJ4",
        "outputId": "1a056fe3-8419-41f4-af86-48b2712c747e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Collection 'Dense-Retrieval' created.\n",
            "âœ… Documents upserted to Qdrant.\n",
            "\n",
            "ğŸ” Top results for query: Why is my order late?\n",
            "- We are experiencing delays in shipping due to weather conditions. (Score: 0.7849)\n",
            "- Shipping may take 5-7 business days during holiday seasons. (Score: 0.7236)\n",
            "- Refunds are processed within 3-5 business days. (Score: 0.6823)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-b276c748e368>:58: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
            "  hits = qdrant.search(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sparse Retrieval RAG**"
      ],
      "metadata": {
        "id": "-OiICdWEci3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# ENV variables\n",
        "QDRANT_HOST = \"https://fd98050b-4928-44e1-9536-66478313e9c5.us-west-1-0.aws.cloud.qdrant.io\"\n",
        "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.EHI0nboatGy3QWLI6mf5tgpoGaLhSeNR9TtoetM16bE\"\n",
        "GEMINI_API_KEY = \"AIzaSyBLWAaYmdhyTHlXAnULNGVxuxx21sq_HBg\"\n",
        "\n",
        "# Initialize Gemini\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Initialize Qdrant client\n",
        "qdrant = QdrantClient(\n",
        "    url=QDRANT_HOST,\n",
        "    api_key=QDRANT_API_KEY,\n",
        ")\n",
        "\n",
        "COLLECTION_NAME = \"Sparse-Retrieval\"\n",
        "\n",
        "# Step 1: Create collection (if not exists)\n",
        "try:\n",
        "    qdrant.get_collection(collection_name=COLLECTION_NAME)\n",
        "    print(f\"âœ… Collection '{COLLECTION_NAME}' already exists.\")\n",
        "except Exception:\n",
        "    qdrant.create_collection(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
        "    )\n",
        "    print(f\"âœ… Collection '{COLLECTION_NAME}' created.\")\n",
        "\n",
        "# Step 2: Sample documents (expanded with a tech-related example)\n",
        "docs = [\n",
        "    {\"id\": \"1\", \"text\": \"We are experiencing delays in shipping due to weather conditions.\"},\n",
        "    {\"id\": \"2\", \"text\": \"Shipping may take 5-7 business days during holiday seasons.\"},\n",
        "    {\"id\": \"3\", \"text\": \"Refunds are processed within 3-5 business days.\"},\n",
        "    {\"id\": \"4\", \"text\": \"ERR_CONN_REFUSED: Check your network settings or firewall.\"},\n",
        "]\n",
        "\n",
        "# Step 3: Embed documents & upload to Qdrant (Dense Retrieval)\n",
        "points = []\n",
        "for doc in docs:\n",
        "    response = genai.embed_content(model=\"models/embedding-001\", content=doc[\"text\"])\n",
        "    embedding = response[\"embedding\"]\n",
        "    points.append(PointStruct(id=int(doc[\"id\"]), vector=embedding, payload={\"text\": doc[\"text\"]}))\n",
        "\n",
        "qdrant.upsert(collection_name=COLLECTION_NAME, points=points)\n",
        "print(\"âœ… Documents upserted to Qdrant for dense retrieval.\")\n",
        "\n",
        "# Step 4: BM25 Setup for Sparse Retrieval\n",
        "# Tokenize documents for BM25 (split text into words)\n",
        "tokenized_docs = [doc[\"text\"].lower().split() for doc in docs]\n",
        "bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "# Step 5: Query\n",
        "query = \"ERR_CONN_REFUSED\"\n",
        "\n",
        "# Step 6: BM25 Sparse Retrieval\n",
        "tokenized_query = query.lower().split()\n",
        "bm25_scores = bm25.get_scores(tokenized_query)\n",
        "\n",
        "# Get top documents with scores\n",
        "bm25_results = [\n",
        "    {\"text\": docs[i][\"text\"], \"score\": bm25_scores[i]}\n",
        "    for i in range(len(docs))\n",
        "    if bm25_scores[i] > 0\n",
        "]\n",
        "bm25_results = sorted(bm25_results, key=lambda x: x[\"score\"], reverse=True)[:3]\n",
        "\n",
        "# Step 7: Dense Retrieval with Qdrant\n",
        "query_response = genai.embed_content(model=\"models/embedding-001\", content=query)\n",
        "query_vector = query_response[\"embedding\"]\n",
        "dense_hits = qdrant.search(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    query_vector=query_vector,\n",
        "    limit=3,\n",
        ")\n",
        "\n",
        "# Step 8: Display Results\n",
        "print(\"\\nğŸ” BM25 Sparse Retrieval Results for query:\", query)\n",
        "for result in bm25_results:\n",
        "    print(f\"- {result['text']} (Score: {result['score']:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ” Dense Retrieval Results for query:\", query)\n",
        "for hit in dense_hits:\n",
        "    print(f\"- {hit.payload['text']} (Score: {hit.score:.4f})\")"
      ],
      "metadata": {
        "id": "WPMvJReoHaR3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "99019cd6-ef17-4a67-b9c7-eb8ac28d0f8d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Collection 'Sparse-Retrieval' created.\n",
            "âœ… Documents upserted to Qdrant for dense retrieval.\n",
            "\n",
            "ğŸ” BM25 Sparse Retrieval Results for query: ERR_CONN_REFUSED\n",
            "\n",
            "ğŸ” Dense Retrieval Results for query: ERR_CONN_REFUSED\n",
            "- ERR_CONN_REFUSED: Check your network settings or firewall. (Score: 0.9169)\n",
            "- We are experiencing delays in shipping due to weather conditions. (Score: 0.6275)\n",
            "- Refunds are processed within 3-5 business days. (Score: 0.5791)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-2ded2588e260>:75: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
            "  dense_hits = qdrant.search(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hybrid Retrieval RAG**"
      ],
      "metadata": {
        "id": "dFTfIG0AfFaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import CrossEncoder\n",
        "import numpy as np\n",
        "\n",
        "# ENV variables\n",
        "QDRANT_HOST = \"https://fd98050b-4928-44e1-9536-66478313e9c5.us-west-1-0.aws.cloud.qdrant.io\"\n",
        "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.EHI0nboatGy3QWLI6mf5tgpoGaLhSeNR9TtoetM16bE\"\n",
        "GEMINI_API_KEY = \"AIzaSyBLWAaYmdhyTHlXAnULNGVxuxx21sq_HBg\"\n",
        "\n",
        "# Initialize Gemini\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Initialize Qdrant client\n",
        "qdrant = QdrantClient(\n",
        "    url=QDRANT_HOST,\n",
        "    api_key=QDRANT_API_KEY,\n",
        ")\n",
        "\n",
        "# Initialize cross-encoder for re-ranking\n",
        "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "COLLECTION_NAME = \"Hybrid-Retrieval\"\n",
        "\n",
        "# Step 1: Create collection (if not exists)\n",
        "try:\n",
        "    qdrant.get_collection(collection_name=COLLECTION_NAME)\n",
        "    print(f\"âœ… Collection '{COLLECTION_NAME}' already exists.\")\n",
        "except Exception:\n",
        "    qdrant.create_collection(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
        "    )\n",
        "    print(f\"âœ… Collection '{COLLECTION_NAME}' created.\")\n",
        "\n",
        "# Step 2: Sample documents (tailored to query)\n",
        "docs = [\n",
        "    {\"id\": \"1\", \"text\": \"The MYC gene regulates cell growth and is influenced by environmental stressors.\"},\n",
        "    {\"id\": \"2\", \"text\": \"Climate adaptation in plants involves genetic changes, including MYC gene expression.\"},\n",
        "    {\"id\": \"3\", \"text\": \"Shipping delays may occur due to extreme weather conditions.\"},\n",
        "    {\"id\": \"4\", \"text\": \"MYC gene mutations are linked to cancer, not climate adaptation.\"},\n",
        "]\n",
        "\n",
        "# Step 3: Embed documents & upload to Qdrant (Dense Retrieval)\n",
        "points = []\n",
        "for doc in docs:\n",
        "    response = genai.embed_content(model=\"models/embedding-001\", content=doc[\"text\"])\n",
        "    embedding = response[\"embedding\"]\n",
        "    points.append(PointStruct(id=int(doc[\"id\"]), vector=embedding, payload={\"text\": doc[\"text\"]}))\n",
        "\n",
        "qdrant.upsert(collection_name=COLLECTION_NAME, points=points)\n",
        "print(\"âœ… Documents upserted to Qdrant for dense retrieval.\")\n",
        "\n",
        "# Step 4: BM25 Setup for Sparse Retrieval\n",
        "tokenized_docs = [doc[\"text\"].lower().split() for doc in docs]\n",
        "bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "# Step 5: Query\n",
        "query = \"MYC gene in climate adaptation\"\n",
        "\n",
        "# Step 6: Dense Retrieval with Qdrant\n",
        "query_response = genai.embed_content(model=\"models/embedding-001\", content=query)\n",
        "query_vector = query_response[\"embedding\"]\n",
        "dense_hits = qdrant.search(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    query_vector=query_vector,\n",
        "    limit=4,  # Retrieve more to allow re-ranking\n",
        ")\n",
        "\n",
        "# Step 7: BM25 Sparse Retrieval\n",
        "tokenized_query = query.lower().split()\n",
        "bm25_scores = bm25.get_scores(tokenized_query)\n",
        "bm25_results = [\n",
        "    {\"text\": docs[i][\"text\"], \"score\": bm25_scores[i]}\n",
        "    for i in range(len(docs))\n",
        "    if bm25_scores[i] > 0\n",
        "]\n",
        "bm25_results = sorted(bm25_results, key=lambda x: x[\"score\"], reverse=True)[:4]\n",
        "\n",
        "# Step 8: Hybrid Retrieval\n",
        "# Normalize scores\n",
        "dense_scores = {hit.payload[\"text\"]: hit.score for hit in dense_hits}\n",
        "bm25_scores = {result[\"text\"]: result[\"score\"] for result in bm25_results}\n",
        "all_texts = set(dense_scores.keys()).union(bm25_scores.keys())\n",
        "\n",
        "max_dense = max(dense_scores.values(), default=1.0)\n",
        "max_bm25 = max(bm25_scores.values(), default=1.0)\n",
        "\n",
        "# Combine scores (weighted: 60% dense, 40% sparse)\n",
        "hybrid_results = {}\n",
        "for text in all_texts:\n",
        "    dense_score = dense_scores.get(text, 0) / max_dense\n",
        "    bm25_score = bm25_scores.get(text, 0) / max_bm25\n",
        "    hybrid_score = 0.6 * dense_score + 0.4 * bm25_score\n",
        "    hybrid_results[text] = hybrid_score\n",
        "\n",
        "# Step 9: Re-ranking with Cross-Encoder\n",
        "rerank_inputs = [[query, text] for text in hybrid_results.keys()]\n",
        "rerank_scores = cross_encoder.predict(rerank_inputs)\n",
        "reranked_results = [\n",
        "    {\"text\": text, \"score\": rerank_scores[i]}\n",
        "    for i, text in enumerate(hybrid_results.keys())\n",
        "]\n",
        "reranked_results = sorted(reranked_results, key=lambda x: x[\"score\"], reverse=True)[:3]\n",
        "\n",
        "# Step 10: Display Results\n",
        "print(\"\\nğŸ” Dense Retrieval Results for query:\", query)\n",
        "for hit in dense_hits:\n",
        "    print(f\"- {hit.payload['text']} (Score: {hit.score:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ” BM25 Sparse Retrieval Results for query:\", query)\n",
        "for result in bm25_results:\n",
        "    print(f\"- {result['text']} (Score: {result['score']:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ” Hybrid Retrieval Results for query:\", query)\n",
        "for text, score in sorted(hybrid_results.items(), key=lambda x: x[1], reverse=True)[:3]:\n",
        "    print(f\"- {text} (Hybrid Score: {score:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ” Re-ranked Hybrid Results for query:\", query)\n",
        "for result in reranked_results:\n",
        "    print(f\"- {result['text']} (Re-ranked Score: {result['score']:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "A1evUXSPfUz1",
        "outputId": "08247506-ac92-4a35-b3b0-0973b28d9683"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Collection 'Hybrid-Retrieval' created.\n",
            "âœ… Documents upserted to Qdrant for dense retrieval.\n",
            "\n",
            "ğŸ” Dense Retrieval Results for query: MYC gene in climate adaptation\n",
            "- MYC gene mutations are linked to cancer, not climate adaptation. (Score: 0.9194)\n",
            "- Climate adaptation in plants involves genetic changes, including MYC gene expression. (Score: 0.9148)\n",
            "- The MYC gene regulates cell growth and is influenced by environmental stressors. (Score: 0.8316)\n",
            "- Shipping delays may occur due to extreme weather conditions. (Score: 0.5368)\n",
            "\n",
            "ğŸ” BM25 Sparse Retrieval Results for query: MYC gene in climate adaptation\n",
            "- Climate adaptation in plants involves genetic changes, including MYC gene expression. (Score: 2.0047)\n",
            "- MYC gene mutations are linked to cancer, not climate adaptation. (Score: 0.3608)\n",
            "- The MYC gene regulates cell growth and is influenced by environmental stressors. (Score: 0.3317)\n",
            "\n",
            "ğŸ” Hybrid Retrieval Results for query: MYC gene in climate adaptation\n",
            "- Climate adaptation in plants involves genetic changes, including MYC gene expression. (Hybrid Score: 0.9970)\n",
            "- MYC gene mutations are linked to cancer, not climate adaptation. (Hybrid Score: 0.6720)\n",
            "- The MYC gene regulates cell growth and is influenced by environmental stressors. (Hybrid Score: 0.6089)\n",
            "\n",
            "ğŸ” Re-ranked Hybrid Results for query: MYC gene in climate adaptation\n",
            "- Climate adaptation in plants involves genetic changes, including MYC gene expression. (Re-ranked Score: 7.5489)\n",
            "- MYC gene mutations are linked to cancer, not climate adaptation. (Re-ranked Score: 7.4767)\n",
            "- The MYC gene regulates cell growth and is influenced by environmental stressors. (Re-ranked Score: 4.5745)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-cfbc900c2e53>:67: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
            "  dense_hits = qdrant.search(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pre-Retrieval RAG**"
      ],
      "metadata": {
        "id": "ccribzmJjkVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import CrossEncoder\n",
        "import numpy as np\n",
        "\n",
        "# ENV variables\n",
        "QDRANT_HOST = \"https://fd98050b-4928-44e1-9536-66478313e9c5.us-west-1-0.aws.cloud.qdrant.io\"\n",
        "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.EHI0nboatGy3QWLI6mf5tgpoGaLhSeNR9TtoetM16bE\"\n",
        "GEMINI_API_KEY = \"AIzaSyBLWAaYmdhyTHlXAnULNGVxuxx21sq_HBg\"\n",
        "\n",
        "# Initialize Gemini\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Initialize Qdrant client\n",
        "qdrant = QdrantClient(\n",
        "    url=QDRANT_HOST,\n",
        "    api_key=QDRANT_API_KEY,\n",
        ")\n",
        "\n",
        "# Initialize cross-encoder for re-ranking\n",
        "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "COLLECTION_NAME = \"Pre-Retrieval\"\n",
        "\n",
        "# Step 1: Create collection (if not exists)\n",
        "try:\n",
        "    qdrant.get_collection(collection_name=COLLECTION_NAME)\n",
        "    print(f\"âœ… Collection '{COLLECTION_NAME}' already exists.\")\n",
        "except Exception:\n",
        "    qdrant.create_collection(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
        "    )\n",
        "    print(f\"âœ… Collection '{COLLECTION_NAME}' created.\")\n",
        "\n",
        "# Step 2: Sample documents (medical context for Medication X)\n",
        "docs = [\n",
        "    {\"id\": \"1\", \"text\": \"Medication X may cause nausea, dizziness, and fatigue as common side effects.\"},\n",
        "    {\"id\": \"2\", \"text\": \"Rare side effects of Medication X include allergic reactions and liver issues.\"},\n",
        "    {\"id\": \"3\", \"text\": \"Medication X is used to treat hypertension but may cause headaches in some patients.\"},\n",
        "    {\"id\": \"4\", \"text\": \"Always consult a doctor before stopping Medication X due to side effects.\"},\n",
        "]\n",
        "\n",
        "# Step 3: Embed documents & upload to Qdrant (Dense Retrieval)\n",
        "points = []\n",
        "for doc in docs:\n",
        "    response = genai.embed_content(model=\"models/embedding-001\", content=doc[\"text\"])\n",
        "    embedding = response[\"embedding\"]\n",
        "    points.append(PointStruct(id=int(doc[\"id\"]), vector=embedding, payload={\"text\": doc[\"text\"]}))\n",
        "\n",
        "qdrant.upsert(collection_name=COLLECTION_NAME, points=points)\n",
        "print(\"âœ… Documents upserted to Qdrant for dense retrieval.\")\n",
        "\n",
        "# Step 4: BM25 Setup for Sparse Retrieval\n",
        "tokenized_docs = [doc[\"text\"].lower().split() for doc in docs]\n",
        "bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "# Step 5: Query\n",
        "query = \"Medication X side effects\"\n",
        "\n",
        "# Step 6: Dense Retrieval with Qdrant (Updated to query_points)\n",
        "query_response = genai.embed_content(model=\"models/embedding-001\", content=query)\n",
        "query_vector = query_response[\"embedding\"]\n",
        "dense_hits = qdrant.query_points(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    query=query_vector,\n",
        "    limit=4,\n",
        "    with_payload=True\n",
        ").points\n",
        "\n",
        "# Step 7: BM25 Sparse Retrieval\n",
        "tokenized_query = query.lower().split()\n",
        "bm25_scores = bm25.get_scores(tokenized_query)\n",
        "bm25_results = [\n",
        "    {\"text\": docs[i][\"text\"], \"score\": bm25_scores[i]}\n",
        "    for i in range(len(docs))\n",
        "    if bm25_scores[i] > 0\n",
        "]\n",
        "bm25_results = sorted(bm25_results, key=lambda x: x[\"score\"], reverse=True)[:4]\n",
        "\n",
        "# Step 8: Hybrid Retrieval\n",
        "dense_scores = {hit.payload[\"text\"]: hit.score for hit in dense_hits}\n",
        "bm25_scores = {result[\"text\"]: result[\"score\"] for result in bm25_results}\n",
        "all_texts = set(dense_scores.keys()).union(bm25_scores.keys())\n",
        "\n",
        "max_dense = max(dense_scores.values(), default=1.0)\n",
        "max_bm25 = max(bm25_scores.values(), default=1.0)\n",
        "\n",
        "hybrid_results = {}\n",
        "for text in all_texts:\n",
        "    dense_score = dense_scores.get(text, 0) / max_dense\n",
        "    bm25_score = bm25_scores.get(text, 0) / max_bm25\n",
        "    hybrid_score = 0.6 * dense_score + 0.4 * bm25_score\n",
        "    hybrid_results[text] = hybrid_score\n",
        "\n",
        "# Step 9: Re-ranking with Cross-Encoder\n",
        "rerank_inputs = [[query, text] for text in hybrid_results.keys()]\n",
        "rerank_scores = cross_encoder.predict(rerank_inputs)\n",
        "reranked_results = [\n",
        "    {\"text\": text, \"score\": rerank_scores[i]}\n",
        "    for i, text in enumerate(hybrid_results.keys())\n",
        "]\n",
        "reranked_results = sorted(reranked_results, key=lambda x: x[\"score\"], reverse=True)[:3]\n",
        "\n",
        "# Step 10: Generate Answer with Gemini (RAG)\n",
        "context = \"\\n\".join([result[\"text\"] for result in reranked_results])\n",
        "prompt = f\"Based on the following context, provide a concise answer to the query: {query}\\n\\nContext:\\n{context}\\n\\nAnswer:\"\n",
        "\n",
        "# Configure Gemini model for generation\n",
        "model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "# Step 11: Display Results\n",
        "print(\"\\nğŸ” Dense Retrieval Results for query:\", query)\n",
        "for hit in dense_hits:\n",
        "    print(f\"- {hit.payload['text']} (Score: {hit.score:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ” BM25 Sparse Retrieval Results for query:\", query)\n",
        "for result in bm25_results:\n",
        "    print(f\"- {result['text']} (Score: {result['score']:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ” Hybrid Retrieval Results for query:\", query)\n",
        "for text, score in sorted(hybrid_results.items(), key=lambda x: x[1], reverse=True)[:3]:\n",
        "    print(f\"- {text} (Hybrid Score: {score:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ” Re-ranked Hybrid Results for query:\", query)\n",
        "for result in reranked_results:\n",
        "    print(f\"- {result['text']} (Re-ranked Score: {result['score']:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ“ Generated Answer:\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "7Z1-83wPjlhT",
        "outputId": "080f311b-8fe3-4844-cf56-c21210a10542"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Collection 'Pre-Retrieval' already exists.\n",
            "âœ… Documents upserted to Qdrant for dense retrieval.\n",
            "\n",
            "ğŸ” Dense Retrieval Results for query: Medication X side effects\n",
            "- Rare side effects of Medication X include allergic reactions and liver issues. (Score: 0.8865)\n",
            "- Medication X may cause nausea, dizziness, and fatigue as common side effects. (Score: 0.8762)\n",
            "- Always consult a doctor before stopping Medication X due to side effects. (Score: 0.8613)\n",
            "- Medication X is used to treat hypertension but may cause headaches in some patients. (Score: 0.8400)\n",
            "\n",
            "ğŸ” BM25 Sparse Retrieval Results for query: Medication X side effects\n",
            "- Rare side effects of Medication X include allergic reactions and liver issues. (Score: 1.2618)\n",
            "- Medication X may cause nausea, dizziness, and fatigue as common side effects. (Score: 0.3990)\n",
            "- Always consult a doctor before stopping Medication X due to side effects. (Score: 0.3990)\n",
            "- Medication X is used to treat hypertension but may cause headaches in some patients. (Score: 0.2478)\n",
            "\n",
            "ğŸ” Hybrid Retrieval Results for query: Medication X side effects\n",
            "- Rare side effects of Medication X include allergic reactions and liver issues. (Hybrid Score: 1.0000)\n",
            "- Medication X may cause nausea, dizziness, and fatigue as common side effects. (Hybrid Score: 0.7195)\n",
            "- Always consult a doctor before stopping Medication X due to side effects. (Hybrid Score: 0.7094)\n",
            "\n",
            "ğŸ” Re-ranked Hybrid Results for query: Medication X side effects\n",
            "- Medication X may cause nausea, dizziness, and fatigue as common side effects. (Re-ranked Score: 9.3250)\n",
            "- Rare side effects of Medication X include allergic reactions and liver issues. (Re-ranked Score: 9.3097)\n",
            "- Medication X is used to treat hypertension but may cause headaches in some patients. (Re-ranked Score: 5.7645)\n",
            "\n",
            "ğŸ“ Generated Answer:\n",
            "Nausea, dizziness, fatigue (common); allergic reactions, liver issues (rare); headaches.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Post-Retrieval RAG**"
      ],
      "metadata": {
        "id": "I4DA_STNlT79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import CrossEncoder\n",
        "import numpy as np\n",
        "\n",
        "# ENV variables\n",
        "QDRANT_HOST = \"https://fd98050b-4928-44e1-9536-66478313e9c5.us-west-1-0.aws.cloud.qdrant.io\"\n",
        "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.EHI0nboatGy3QWLI6mf5tgpoGaLhSeNR9TtoetM16bE\"\n",
        "GEMINI_API_KEY = \"AIzaSyBLWAaYmdhyTHlXAnULNGVxuxx21sq_HBg\"\n",
        "\n",
        "# Initialize Gemini\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Initialize Qdrant client\n",
        "qdrant = QdrantClient(\n",
        "    url=QDRANT_HOST,\n",
        "    api_key=QDRANT_API_KEY,\n",
        ")\n",
        "\n",
        "# Initialize cross-encoder for re-ranking\n",
        "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "COLLECTION_NAME = \"Post-Retrieval\"\n",
        "\n",
        "# Step 1: Create collection (if not exists)\n",
        "try:\n",
        "    qdrant.get_collection(collection_name=COLLECTION_NAME)\n",
        "    print(f\"âœ… Collection '{COLLECTION_NAME}' already exists.\")\n",
        "except Exception:\n",
        "    qdrant.create_collection(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
        "    )\n",
        "    print(f\"âœ… Collection '{COLLECTION_NAME}' created.\")\n",
        "\n",
        "# Step 2: Sample documents (Mars habitability context)\n",
        "docs = [\n",
        "    {\"id\": \"1\", \"text\": \"Mars habitability is limited by its thin atmosphere and lack of liquid water.\"},\n",
        "    {\"id\": \"2\", \"text\": \"Evidence of ancient water flows on Mars suggests past habitability.\"},\n",
        "    {\"id\": \"3\", \"text\": \"Current Mars missions search for microbial life in subsurface ice.\"},\n",
        "    {\"id\": \"4\", \"text\": \"Terraforming Mars could make it habitable, but technology is decades away.\"},\n",
        "]\n",
        "\n",
        "# Step 3: Embed documents & upload to Qdrant (Dense Retrieval)\n",
        "points = []\n",
        "for doc in docs:\n",
        "    response = genai.embed_content(model=\"models/embedding-001\", content=doc[\"text\"])\n",
        "    embedding = response[\"embedding\"]\n",
        "    points.append(PointStruct(id=int(doc[\"id\"]), vector=embedding, payload={\"text\": doc[\"text\"]}))\n",
        "\n",
        "qdrant.upsert(collection_name=COLLECTION_NAME, points=points)\n",
        "print(\"âœ… Documents upserted to Qdrant for dense retrieval.\")\n",
        "\n",
        "# Step 4: BM25 Setup for Sparse Retrieval\n",
        "tokenized_docs = [doc[\"text\"].lower().split() for doc in docs]\n",
        "bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "# Step 5: Query\n",
        "query = \"Mars habitability\"\n",
        "\n",
        "# Step 6: Initial Answer Generation\n",
        "initial_prompt = f\"Provide a brief answer to the query: {query}\"\n",
        "model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
        "initial_response = model.generate_content(initial_prompt)\n",
        "initial_answer = initial_response.text\n",
        "\n",
        "# Step 7: Dense Retrieval with Qdrant\n",
        "query_response = genai.embed_content(model=\"models/embedding-001\", content=query)\n",
        "query_vector = query_response[\"embedding\"]\n",
        "dense_hits = qdrant.query_points(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    query=query_vector,\n",
        "    limit=4,\n",
        "    with_payload=True\n",
        ").points\n",
        "\n",
        "# Step 8: BM25 Sparse Retrieval\n",
        "tokenized_query = query.lower().split()\n",
        "bm25_scores = bm25.get_scores(tokenized_query)\n",
        "bm25_results = [\n",
        "    {\"text\": docs[i][\"text\"], \"score\": bm25_scores[i]}\n",
        "    for i in range(len(docs))\n",
        "    if bm25_scores[i] > 0\n",
        "]\n",
        "bm25_results = sorted(bm25_results, key=lambda x: x[\"score\"], reverse=True)[:4]\n",
        "\n",
        "# Step 9: Hybrid Retrieval\n",
        "dense_scores = {hit.payload[\"text\"]: hit.score for hit in dense_hits}\n",
        "bm25_scores = {result[\"text\"]: result[\"score\"] for result in bm25_results}\n",
        "all_texts = set(dense_scores.keys()).union(bm25_scores.keys())\n",
        "\n",
        "max_dense = max(dense_scores.values(), default=1.0)\n",
        "max_bm25 = max(bm25_scores.values(), default=1.0)\n",
        "\n",
        "hybrid_results = {}\n",
        "for text in all_texts:\n",
        "    dense_score = dense_scores.get(text, 0) / max_dense\n",
        "    bm25_score = bm25_scores.get(text, 0) / max_bm25\n",
        "    hybrid_score = 0.6 * dense_score + 0.4 * bm25_score\n",
        "    hybrid_results[text] = hybrid_score\n",
        "\n",
        "# Step 10: Re-ranking with Cross-Encoder\n",
        "rerank_inputs = [[query, text] for text in hybrid_results.keys()]\n",
        "rerank_scores = cross_encoder.predict(rerank_inputs)\n",
        "reranked_results = [\n",
        "    {\"text\": text, \"score\": rerank_scores[i]}\n",
        "    for i, text in enumerate(hybrid_results.keys())\n",
        "]\n",
        "reranked_results = sorted(reranked_results, key=lambda x: x[\"score\"], reverse=True)[:3]\n",
        "\n",
        "# Step 11: Refine Answer with Retrieved Evidence\n",
        "context = \"\\n\".join([result[\"text\"] for result in reranked_results])\n",
        "refine_prompt = f\"\"\"\n",
        "Query: {query}\n",
        "Initial Answer: {initial_answer}\n",
        "Context: {context}\n",
        "\n",
        "Refine the initial answer based on the provided context to ensure accuracy and include relevant details. If the initial answer contains inaccuracies, correct them. Provide a concise, factual response.\n",
        "Answer:\n",
        "\"\"\"\n",
        "refined_response = model.generate_content(refine_prompt)\n",
        "refined_answer = refined_response.text\n",
        "\n",
        "# Step 12: Display Results\n",
        "print(\"\\nğŸ“ Initial Answer:\")\n",
        "print(initial_answer)\n",
        "\n",
        "print(\"\\nğŸ” Dense Retrieval Results for query:\", query)\n",
        "for hit in dense_hits:\n",
        "    print(f\"- {hit.payload['text']} (Score: {hit.score:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ” BM25 Sparse Retrieval Results for query:\", query)\n",
        "for result in bm25_results:\n",
        "    print(f\"- {result['text']} (Score: {result['score']:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ” Hybrid Retrieval Results for query:\", query)\n",
        "for text, score in sorted(hybrid_results.items(), key=lambda x: x[1], reverse=True)[:3]:\n",
        "    print(f\"- {text} (Hybrid Score: {score:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ” Re-ranked Hybrid Results for query:\", query)\n",
        "for result in reranked_results:\n",
        "    print(f\"- {result['text']} (Re-ranked Score: {result['score']:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ“ Refined Answer:\")\n",
        "print(refined_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "f9NuDWE6lWLL",
        "outputId": "f47b3f63-49af-4013-a10c-cd29a18fb0a1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Collection 'Post-Retrieval' created.\n",
            "âœ… Documents upserted to Qdrant for dense retrieval.\n",
            "\n",
            "ğŸ“ Initial Answer:\n",
            "Mars's past habitability is highly probable, with evidence of liquid water.  Current habitability is unlikely without significant terraforming due to thin atmosphere, high radiation, and lack of readily available liquid water.\n",
            "\n",
            "\n",
            "ğŸ” Dense Retrieval Results for query: Mars habitability\n",
            "- Mars habitability is limited by its thin atmosphere and lack of liquid water. (Score: 0.8981)\n",
            "- Evidence of ancient water flows on Mars suggests past habitability. (Score: 0.7998)\n",
            "- Terraforming Mars could make it habitable, but technology is decades away. (Score: 0.7916)\n",
            "- Current Mars missions search for microbial life in subsurface ice. (Score: 0.7196)\n",
            "\n",
            "ğŸ” BM25 Sparse Retrieval Results for query: Mars habitability\n",
            "- Mars habitability is limited by its thin atmosphere and lack of liquid water. (Score: 0.9509)\n",
            "- Evidence of ancient water flows on Mars suggests past habitability. (Score: 0.1892)\n",
            "- Current Mars missions search for microbial life in subsurface ice. (Score: 0.1892)\n",
            "- Terraforming Mars could make it habitable, but technology is decades away. (Score: 0.1814)\n",
            "\n",
            "ğŸ” Hybrid Retrieval Results for query: Mars habitability\n",
            "- Mars habitability is limited by its thin atmosphere and lack of liquid water. (Hybrid Score: 1.0000)\n",
            "- Evidence of ancient water flows on Mars suggests past habitability. (Hybrid Score: 0.6139)\n",
            "- Terraforming Mars could make it habitable, but technology is decades away. (Hybrid Score: 0.6051)\n",
            "\n",
            "ğŸ” Re-ranked Hybrid Results for query: Mars habitability\n",
            "- Mars habitability is limited by its thin atmosphere and lack of liquid water. (Re-ranked Score: 9.1104)\n",
            "- Evidence of ancient water flows on Mars suggests past habitability. (Re-ranked Score: 5.7168)\n",
            "- Terraforming Mars could make it habitable, but technology is decades away. (Re-ranked Score: 4.2530)\n",
            "\n",
            "ğŸ“ Refined Answer:\n",
            "Mars once likely had a thicker atmosphere and liquid water, evidenced by ancient riverbeds and other geological features, suggesting past habitability.  Its current thin atmosphere provides little protection from harmful radiation, and liquid water is scarce, making the planet uninhabitable for humans without significant terraforming, a technology far from current capabilities.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Iterative Retrieval RAG**\n"
      ],
      "metadata": {
        "id": "7Da0MC17lpl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import CrossEncoder\n",
        "import numpy as np\n",
        "\n",
        "# ENV variables\n",
        "QDRANT_HOST = \"https://fd98050b-4928-44e1-9536-66478313e9c5.us-west-1-0.aws.cloud.qdrant.io\"\n",
        "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.EHI0nboatGy3QWLI6mf5tgpoGaLhSeNR9TtoetM16bE\"\n",
        "GEMINI_API_KEY = \"AIzaSyBLWAaYmdhyTHlXAnULNGVxuxx21sq_HBg\"\n",
        "\n",
        "# Initialize Gemini\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Initialize Qdrant client\n",
        "qdrant = QdrantClient(\n",
        "    url=QDRANT_HOST,\n",
        "    api_key=QDRANT_API_KEY,\n",
        ")\n",
        "\n",
        "# Initialize cross-encoder for re-ranking\n",
        "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "COLLECTION_NAME = \"Iterative-Retrival\"\n",
        "\n",
        "# Step 1: Create collection (if not exists)\n",
        "try:\n",
        "    qdrant.get_collection(collection_name=COLLECTION_NAME)\n",
        "    print(f\"âœ… Collection '{COLLECTION_NAME}' already exists.\")\n",
        "except Exception:\n",
        "    qdrant.create_collection(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
        "    )\n",
        "    print(f\"âœ… Collection '{COLLECTION_NAME}' created.\")\n",
        "\n",
        "# Step 2: Sample documents (renewable energy in Germany and California)\n",
        "docs = [\n",
        "    {\"id\": \"1\", \"text\": \"Germanyâ€™s renewable energy mix includes 46% wind and solar in 2023, driven by Energiewende policies.\"},\n",
        "    {\"id\": \"2\", \"text\": \"California aims for 60% renewable energy by 2030, with heavy investment in solar farms.\"},\n",
        "    {\"id\": \"3\", \"text\": \"Germanyâ€™s feed-in tariffs have boosted solar and wind adoption since the 2000s.\"},\n",
        "    {\"id\": \"4\", \"text\": \"Californiaâ€™s renewable energy faces grid reliability challenges due to solar intermittency.\"},\n",
        "    {\"id\": \"5\", \"text\": \"Germany leads in offshore wind, while California focuses on rooftop solar.\"},\n",
        "]\n",
        "\n",
        "# Step 3: Embed documents & upload to Qdrant (Dense Retrieval)\n",
        "points = []\n",
        "for doc in docs:\n",
        "    response = genai.embed_content(model=\"models/embedding-001\", content=doc[\"text\"])\n",
        "    embedding = response[\"embedding\"]\n",
        "    points.append(PointStruct(id=int(doc[\"id\"]), vector=embedding, payload={\"text\": doc[\"text\"]}))\n",
        "\n",
        "qdrant.upsert(collection_name=COLLECTION_NAME, points=points)\n",
        "print(\"âœ… Documents upserted to Qdrant for dense retrieval.\")\n",
        "\n",
        "# Step 4: BM25 Setup for Sparse Retrieval\n",
        "tokenized_docs = [doc[\"text\"].lower().split() for doc in docs]\n",
        "bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "# Step 5: Query and Sub-Queries\n",
        "main_query = \"comparing renewable energy in Germany vs. California\"\n",
        "sub_queries = [\n",
        "    \"renewable energy in Germany\",\n",
        "    \"renewable energy in California\",\n",
        "    \"comparison of renewable energy in Germany and California\"\n",
        "]\n",
        "\n",
        "# Step 6: Iterative Retrieval-Generation\n",
        "model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
        "partial_answers = []\n",
        "\n",
        "for cycle, sub_query in enumerate(sub_queries, 1):\n",
        "    print(f\"\\nğŸ”„ Cycle {cycle}: Processing sub-query: {sub_query}\")\n",
        "\n",
        "    # Step 6.1: Dense Retrieval with Qdrant\n",
        "    query_response = genai.embed_content(model=\"models/embedding-001\", content=sub_query)\n",
        "    query_vector = query_response[\"embedding\"]\n",
        "    dense_hits = qdrant.query_points(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        query=query_vector,\n",
        "        limit=4,\n",
        "        with_payload=True\n",
        "    ).points\n",
        "\n",
        "    # Step 6.2: BM25 Sparse Retrieval\n",
        "    tokenized_query = sub_query.lower().split()\n",
        "    bm25_scores = bm25.get_scores(tokenized_query)\n",
        "    bm25_results = [\n",
        "        {\"text\": docs[i][\"text\"], \"score\": bm25_scores[i]}\n",
        "        for i in range(len(docs))\n",
        "        if bm25_scores[i] > 0\n",
        "    ]\n",
        "    bm25_results = sorted(bm25_results, key=lambda x: x[\"score\"], reverse=True)[:4]\n",
        "\n",
        "    # Step 6.3: Hybrid Retrieval\n",
        "    dense_scores = {hit.payload[\"text\"]: hit.score for hit in dense_hits}\n",
        "    bm25_scores = {result[\"text\"]: result[\"score\"] for result in bm25_results}\n",
        "    all_texts = set(dense_scores.keys()).union(bm25_scores.keys())\n",
        "\n",
        "    max_dense = max(dense_scores.values(), default=1.0)\n",
        "    max_bm25 = max(bm25_scores.values(), default=1.0)\n",
        "\n",
        "    hybrid_results = {}\n",
        "    for text in all_texts:\n",
        "        dense_score = dense_scores.get(text, 0) / max_dense\n",
        "        bm25_score = bm25_scores.get(text, 0) / max_bm25\n",
        "        hybrid_score = 0.6 * dense_score + 0.4 * bm25_score\n",
        "        hybrid_results[text] = hybrid_score\n",
        "\n",
        "    # Step 6.4: Re-ranking with Cross-Encoder\n",
        "    rerank_inputs = [[sub_query, text] for text in hybrid_results.keys()]\n",
        "    rerank_scores = cross_encoder.predict(rerank_inputs)\n",
        "    reranked_results = [\n",
        "        {\"text\": text, \"score\": rerank_scores[i]}\n",
        "        for i, text in enumerate(hybrid_results.keys())\n",
        "    ]\n",
        "    reranked_results = sorted(reranked_results, key=lambda x: x[\"score\"], reverse=True)[:3]\n",
        "\n",
        "    # Step 6.5: Generate Partial Answer\n",
        "    context = \"\\n\".join([result[\"text\"] for result in reranked_results])\n",
        "    prompt = f\"\"\"\n",
        "    Based on the following context, provide a concise answer to the query: {sub_query}\n",
        "    Context: {context}\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "    response = model.generate_content(prompt)\n",
        "    partial_answer = response.text\n",
        "    partial_answers.append(partial_answer)\n",
        "\n",
        "    # Display Cycle Results\n",
        "    print(f\"\\nğŸ” Re-ranked Hybrid Results for sub-query: {sub_query}\")\n",
        "    for result in reranked_results:\n",
        "        print(f\"- {result['text']} (Re-ranked Score: {result['score']:.4f})\")\n",
        "    print(f\"\\nğŸ“ Partial Answer: {partial_answer}\")\n",
        "\n",
        "# Step 7: Final Synthesis\n",
        "synthesis_prompt = f\"\"\"\n",
        "Query: {main_query}\n",
        "Partial Answers:\n",
        "1. {partial_answers[0]}\n",
        "2. {partial_answers[1]}\n",
        "3. {partial_answers[2]}\n",
        "\n",
        "Synthesize the partial answers into a comprehensive, concise response comparing renewable energy in Germany and California. Highlight key similarities, differences, and notable policies or challenges.\n",
        "Answer:\n",
        "\"\"\"\n",
        "final_response = model.generate_content(synthesis_prompt)\n",
        "final_answer = final_response.text\n",
        "\n",
        "# Step 8: Display Final Answer\n",
        "print(\"\\nğŸ“ Final Synthesized Answer:\")\n",
        "print(final_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "R_8yMGbNlsYh",
        "outputId": "1bd4e70b-3735-488b-cd10-649c999e8bf9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Collection 'Iterative-Retrival' created.\n",
            "âœ… Documents upserted to Qdrant for dense retrieval.\n",
            "\n",
            "ğŸ”„ Cycle 1: Processing sub-query: renewable energy in Germany\n",
            "\n",
            "ğŸ” Re-ranked Hybrid Results for sub-query: renewable energy in Germany\n",
            "- Germanyâ€™s renewable energy mix includes 46% wind and solar in 2023, driven by Energiewende policies. (Re-ranked Score: 8.7021)\n",
            "- Germanyâ€™s feed-in tariffs have boosted solar and wind adoption since the 2000s. (Re-ranked Score: 0.9584)\n",
            "- Germany leads in offshore wind, while California focuses on rooftop solar. (Re-ranked Score: -0.6066)\n",
            "\n",
            "ğŸ“ Partial Answer: Germany's renewable energy relies heavily on wind and solar, comprising 46% of its mix in 2023, driven by the Energiewende policy promoting renewables.\n",
            "\n",
            "\n",
            "ğŸ”„ Cycle 2: Processing sub-query: renewable energy in California\n",
            "\n",
            "ğŸ” Re-ranked Hybrid Results for sub-query: renewable energy in California\n",
            "- California aims for 60% renewable energy by 2030, with heavy investment in solar farms. (Re-ranked Score: 8.3120)\n",
            "- Californiaâ€™s renewable energy faces grid reliability challenges due to solar intermittency. (Re-ranked Score: 7.2486)\n",
            "- Germany leads in offshore wind, while California focuses on rooftop solar. (Re-ranked Score: -1.7399)\n",
            "\n",
            "ğŸ“ Partial Answer: California is pursuing 60% renewable energy by 2030, primarily through solar (both rooftop and large-scale farms), but faces grid reliability issues due to solar's intermittent nature.\n",
            "\n",
            "\n",
            "ğŸ”„ Cycle 3: Processing sub-query: comparison of renewable energy in Germany and California\n",
            "\n",
            "ğŸ” Re-ranked Hybrid Results for sub-query: comparison of renewable energy in Germany and California\n",
            "- Germany leads in offshore wind, while California focuses on rooftop solar. (Re-ranked Score: 4.1860)\n",
            "- Germanyâ€™s renewable energy mix includes 46% wind and solar in 2023, driven by Energiewende policies. (Re-ranked Score: 2.1502)\n",
            "- California aims for 60% renewable energy by 2030, with heavy investment in solar farms. (Re-ranked Score: 0.4660)\n",
            "\n",
            "ğŸ“ Partial Answer: Germany emphasizes wind energy, particularly offshore, while California prioritizes solar, both rooftop and utility-scale.  Germany has a higher current renewable penetration.\n",
            "\n",
            "\n",
            "ğŸ“ Final Synthesized Answer:\n",
            "Both Germany and California are aggressively pursuing renewable energy transitions, but with differing approaches and challenges.  Germany, with its Energiewende policy, has achieved a higher current renewable penetration (46% in 2023), largely driven by wind power, including significant offshore development.  California, targeting 60% renewables by 2030, relies predominantly on solar power, both rooftop and utility-scale. While both regions utilize solar and wind, Germany emphasizes wind while California prioritizes solar.  A key challenge for California is maintaining grid reliability given the intermittent nature of solar power, a concern less prominent in Germany's more diversified wind-heavy mix.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extractive RAG**"
      ],
      "metadata": {
        "id": "13w4RxOPmlp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import CrossEncoder\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# ENV variables\n",
        "QDRANT_HOST = \"https://fd98050b-4928-44e1-9536-66478313e9c5.us-west-1-0.aws.cloud.qdrant.io\"\n",
        "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.EHI0nboatGy3QWLI6mf5tgpoGaLhSeNR9TtoetM16bE\"\n",
        "GEMINI_API_KEY = \"AIzaSyBLWAaYmdhyTHlXAnULNGVxuxx21sq_HBg\"\n",
        "\n",
        "# Initialize Gemini\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Initialize Qdrant client\n",
        "qdrant = QdrantClient(\n",
        "    url=QDRANT_HOST,\n",
        "    api_key=QDRANT_API_KEY,\n",
        ")\n",
        "\n",
        "# Initialize cross-encoder for re-ranking\n",
        "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "COLLECTION_NAME = \"Extractive\"\n",
        "\n",
        "# Step 1: Create collection (if not exists)\n",
        "try:\n",
        "    qdrant.get_collection(collection_name=COLLECTION_NAME)\n",
        "    print(f\"âœ… Collection '{COLLECTION_NAME}' already exists.\")\n",
        "except Exception:\n",
        "    qdrant.create_collection(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
        "    )\n",
        "    print(f\"âœ… Collection '{COLLECTION_NAME}' created.\")\n",
        "\n",
        "# Step 2: Sample documents (osmosis context)\n",
        "docs = [\n",
        "    {\"id\": \"1\", \"text\": \"Osmosis is the diffusion of water molecules across a selectively permeable membrane from an area of higher water concentration to an area of lower water concentration.\"},\n",
        "    {\"id\": \"2\", \"text\": \"In biology, osmosis plays a critical role in maintaining cell hydration and nutrient transport.\"},\n",
        "    {\"id\": \"3\", \"text\": \"Osmosis differs from active transport, which requires energy to move substances against a concentration gradient.\"},\n",
        "    {\"id\": \"4\", \"text\": \"The process of osmosis is essential for plant roots to absorb water from the soil.\"},\n",
        "]\n",
        "\n",
        "# Step 3: Embed documents & upload to Qdrant (Dense Retrieval)\n",
        "points = []\n",
        "for doc in docs:\n",
        "    response = genai.embed_content(model=\"models/embedding-001\", content=doc[\"text\"])\n",
        "    embedding = response[\"embedding\"]\n",
        "    points.append(PointStruct(id=int(doc[\"id\"]), vector=embedding, payload={\"text\": doc[\"text\"]}))\n",
        "\n",
        "qdrant.upsert(collection_name=COLLECTION_NAME, points=points)\n",
        "print(\"âœ… Documents upserted to Qdrant for dense retrieval.\")\n",
        "\n",
        "# Step 4: BM25 Setup for Sparse Retrieval\n",
        "tokenized_docs = [doc[\"text\"].lower().split() for doc in docs]\n",
        "bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "# Step 5: Query\n",
        "query = \"osmosis definition\"\n",
        "\n",
        "# Step 6: Dense Retrieval with Qdrant\n",
        "query_response = genai.embed_content(model=\"models/embedding-001\", content=query)\n",
        "query_vector = query_response[\"embedding\"]\n",
        "dense_hits = qdrant.query_points(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    query=query_vector,\n",
        "    limit=4,\n",
        "    with_payload=True\n",
        ").points\n",
        "\n",
        "# Step 7: BM25 Sparse Retrieval\n",
        "tokenized_query = query.lower().split()\n",
        "bm25_scores = bm25.get_scores(tokenized_query)\n",
        "bm25_results = [\n",
        "    {\"text\": docs[i][\"text\"], \"score\": bm25_scores[i]}\n",
        "    for i in range(len(docs))\n",
        "    if bm25_scores[i] > 0\n",
        "]\n",
        "bm25_results = sorted(bm25_results, key=lambda x: x[\"score\"], reverse=True)[:4]\n",
        "\n",
        "# Step 8: Hybrid Retrieval\n",
        "dense_scores = {hit.payload[\"text\"]: hit.score for hit in dense_hits}\n",
        "bm25_scores = {result[\"text\"]: result[\"score\"] for result in bm25_results}\n",
        "all_texts = set(dense_scores.keys()).union(bm25_scores.keys())\n",
        "\n",
        "max_dense = max(dense_scores.values(), default=1.0)\n",
        "max_bm25 = max(bm25_scores.values(), default=1.0)\n",
        "\n",
        "hybrid_results = {}\n",
        "for text in all_texts:\n",
        "    dense_score = dense_scores.get(text, 0) / max_dense\n",
        "    bm25_score = bm25_scores.get(text, 0) / max_bm25\n",
        "    hybrid_score = 0.6 * dense_score + 0.4 * bm25_score\n",
        "    hybrid_results[text] = hybrid_score\n",
        "\n",
        "# Step 9: Re-ranking with Cross-Encoder\n",
        "rerank_inputs = [[query, text] for text in hybrid_results.keys()]\n",
        "rerank_scores = cross_encoder.predict(rerank_inputs)\n",
        "reranked_results = [\n",
        "    {\"text\": text, \"score\": rerank_scores[i]}\n",
        "    for i, text in enumerate(hybrid_results.keys())\n",
        "]\n",
        "reranked_results = sorted(reranked_results, key=lambda x: x[\"score\"], reverse=True)[:3]\n",
        "\n",
        "# Step 10: Snippet Extraction\n",
        "# Extract sentences containing \"osmosis\" and rank by relevance to \"definition\"\n",
        "snippets = []\n",
        "for result in reranked_results:\n",
        "    text = result[\"text\"]\n",
        "    # Split text into sentences\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    for sentence in sentences:\n",
        "        if \"osmosis\" in sentence.lower() and any(word in sentence.lower() for word in [\"is\", \"defined\", \"definition\"]):\n",
        "            snippets.append({\"text\": sentence.strip(), \"score\": result[\"score\"]})\n",
        "\n",
        "# Sort snippets by score and select the top one (or more if needed)\n",
        "snippets = sorted(snippets, key=lambda x: x[\"score\"], reverse=True)[:1]\n",
        "\n",
        "# Step 11: Optional Validation with Gemini\n",
        "# Use Gemini to format or confirm the snippet (minimal generation)\n",
        "model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
        "if snippets:\n",
        "    snippet_text = snippets[0][\"text\"]\n",
        "    prompt = f\"\"\"\n",
        "    Query: {query}\n",
        "    Extracted Snippet: {snippet_text}\n",
        "\n",
        "    Format the snippet as a quoted definition for the query. Ensure the text remains verbatim and add minimal context if needed.\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "    response = model.generate_content(prompt)\n",
        "    final_answer = response.text\n",
        "else:\n",
        "    final_answer = \"No exact definition of osmosis found in the provided documents.\"\n",
        "\n",
        "# Step 12: Display Results\n",
        "print(\"\\nğŸ” Dense Retrieval Results for query:\", query)\n",
        "for hit in dense_hits:\n",
        "    print(f\"- {hit.payload['text']} (Score: {hit.score:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ” BM25 Sparse Retrieval Results for query:\", query)\n",
        "for result in bm25_results:\n",
        "    print(f\"- {result['text']} (Score: {result['score']:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ” Hybrid Retrieval Results for query:\", query)\n",
        "for text, score in sorted(hybrid_results.items(), key=lambda x: x[1], reverse=True)[:3]:\n",
        "    print(f\"- {text} (Hybrid Score: {score:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ” Re-ranked Hybrid Results for query:\", query)\n",
        "for result in reranked_results:\n",
        "    print(f\"- {result['text']} (Re-ranked Score: {result['score']:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ“ Extracted Answer:\")\n",
        "print(final_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "K6ZuIXn0l-of",
        "outputId": "375b4cbc-e174-4e8d-d893-970826823fed"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Collection 'Extractive' created.\n",
            "âœ… Documents upserted to Qdrant for dense retrieval.\n",
            "\n",
            "ğŸ” Dense Retrieval Results for query: osmosis definition\n",
            "- Osmosis is the diffusion of water molecules across a selectively permeable membrane from an area of higher water concentration to an area of lower water concentration. (Score: 0.8940)\n",
            "- The process of osmosis is essential for plant roots to absorb water from the soil. (Score: 0.8441)\n",
            "- In biology, osmosis plays a critical role in maintaining cell hydration and nutrient transport. (Score: 0.8260)\n",
            "- Osmosis differs from active transport, which requires energy to move substances against a concentration gradient. (Score: 0.8155)\n",
            "\n",
            "ğŸ” BM25 Sparse Retrieval Results for query: osmosis definition\n",
            "- In biology, osmosis plays a critical role in maintaining cell hydration and nutrient transport. (Score: 0.1620)\n",
            "- Osmosis differs from active transport, which requires energy to move substances against a concentration gradient. (Score: 0.1576)\n",
            "- The process of osmosis is essential for plant roots to absorb water from the soil. (Score: 0.1576)\n",
            "- Osmosis is the diffusion of water molecules across a selectively permeable membrane from an area of higher water concentration to an area of lower water concentration. (Score: 0.1210)\n",
            "\n",
            "ğŸ” Hybrid Retrieval Results for query: osmosis definition\n",
            "- The process of osmosis is essential for plant roots to absorb water from the soil. (Hybrid Score: 0.9555)\n",
            "- In biology, osmosis plays a critical role in maintaining cell hydration and nutrient transport. (Hybrid Score: 0.9544)\n",
            "- Osmosis differs from active transport, which requires energy to move substances against a concentration gradient. (Hybrid Score: 0.9363)\n",
            "\n",
            "ğŸ” Re-ranked Hybrid Results for query: osmosis definition\n",
            "- Osmosis is the diffusion of water molecules across a selectively permeable membrane from an area of higher water concentration to an area of lower water concentration. (Re-ranked Score: 9.2938)\n",
            "- The process of osmosis is essential for plant roots to absorb water from the soil. (Re-ranked Score: 5.3194)\n",
            "- In biology, osmosis plays a critical role in maintaining cell hydration and nutrient transport. (Re-ranked Score: 4.3938)\n",
            "\n",
            "ğŸ“ Extracted Answer:\n",
            "Osmosis is defined as \"the diffusion of water molecules across a selectively permeable membrane from an area of higher water concentration to an area of lower water concentration.\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Abstractive RAG**"
      ],
      "metadata": {
        "id": "IHD8S3_onuLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import CrossEncoder\n",
        "import numpy as np\n",
        "\n",
        "# ENV variables\n",
        "QDRANT_HOST = \"https://fd98050b-4928-44e1-9536-66478313e9c5.us-west-1-0.aws.cloud.qdrant.io\"\n",
        "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.EHI0nboatGy3QWLI6mf5tgpoGaLhSeNR9TtoetM16bE\"\n",
        "GEMINI_API_KEY = \"AIzaSyBLWAaYmdhyTHlXAnULNGVxuxx21sq_HBg\"\n",
        "\n",
        "# Initialize Gemini\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Initialize Qdrant client\n",
        "qdrant = QdrantClient(\n",
        "    url=QDRANT_HOST,\n",
        "    api_key=QDRANT_API_KEY,\n",
        ")\n",
        "\n",
        "# Initialize cross-encoder for re-ranking\n",
        "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "COLLECTION_NAME = \"Abstractive\"\n",
        "\n",
        "# Step 1: Create collection (if not exists)\n",
        "try:\n",
        "    qdrant.get_collection(collection_name=COLLECTION_NAME)\n",
        "    print(f\"âœ… Collection '{COLLECTION_NAME}' already exists.\")\n",
        "except Exception:\n",
        "    qdrant.create_collection(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
        "    )\n",
        "    print(f\"âœ… Collection '{COLLECTION_NAME}' created.\")\n",
        "\n",
        "# Step 2: Sample documents (news articles on AI advancements in 2025)\n",
        "docs = [\n",
        "    {\"id\": \"1\", \"text\": \"In 2025, AI models achieved breakthroughs in multimodal processing, integrating text, images, and audio for applications in healthcare and autonomous vehicles.\"},\n",
        "    {\"id\": \"2\", \"text\": \"Major tech companies in 2025 invested heavily in quantum AI, promising faster computation for complex problems like climate modeling.\"},\n",
        "    {\"id\": \"3\", \"text\": \"Ethical AI frameworks gained traction in 2025, with new regulations in Europe to ensure transparency in AI decision-making.\"},\n",
        "    {\"id\": \"4\", \"text\": \"AI-driven personalized education platforms expanded in 2025, tailoring curricula to individual student needs.\"},\n",
        "]\n",
        "\n",
        "# Step 3: Embed documents & upload to Qdrant (Dense Retrieval)\n",
        "points = []\n",
        "for doc in docs:\n",
        "    response = genai.embed_content(model=\"models/embedding-001\", content=doc[\"text\"])\n",
        "    embedding = response[\"embedding\"]\n",
        "    points.append(PointStruct(id=int(doc[\"id\"]), vector=embedding, payload={\"text\": doc[\"text\"]}))\n",
        "\n",
        "qdrant.upsert(collection_name=COLLECTION_NAME, points=points)\n",
        "print(\"âœ… Documents upserted to Qdrant for dense retrieval.\")\n",
        "\n",
        "# Step 4: BM25 Setup for Sparse Retrieval\n",
        "tokenized_docs = [doc[\"text\"].lower().split() for doc in docs]\n",
        "bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "# Step 5: Query\n",
        "query = \"summarize AI advancements in 2025\"\n",
        "\n",
        "# Step 6: Dense Retrieval with Qdrant\n",
        "query_response = genai.embed_content(model=\"models/embedding-001\", content=query)\n",
        "query_vector = query_response[\"embedding\"]\n",
        "dense_hits = qdrant.query_points(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    query=query_vector,\n",
        "    limit=4,\n",
        "    with_payload=True\n",
        ").points\n",
        "\n",
        "# Step 7: BM25 Sparse Retrieval\n",
        "tokenized_query = query.lower().split()\n",
        "bm25_scores = bm25.get_scores(tokenized_query)\n",
        "bm25_results = [\n",
        "    {\"text\": docs[i][\"text\"], \"score\": bm25_scores[i]}\n",
        "    for i in range(len(docs))\n",
        "    if bm25_scores[i] > 0\n",
        "]\n",
        "bm25_results = sorted(bm25_results, key=lambda x: x[\"score\"], reverse=True)[:4]\n",
        "\n",
        "# Step 8: Hybrid Retrieval\n",
        "dense_scores = {hit.payload[\"text\"]: hit.score for hit in dense_hits}\n",
        "bm25_scores = {result[\"text\"]: result[\"score\"] for result in bm25_results}\n",
        "all_texts = set(dense_scores.keys()).union(bm25_scores.keys())\n",
        "\n",
        "max_dense = max(dense_scores.values(), default=1.0)\n",
        "max_bm25 = max(bm25_scores.values(), default=1.0)\n",
        "\n",
        "hybrid_results = {}\n",
        "for text in all_texts:\n",
        "    dense_score = dense_scores.get(text, 0) / max_dense\n",
        "    bm25_score = bm25_scores.get(text, 0) / max_bm25\n",
        "    hybrid_score = 0.6 * dense_score + 0.4 * bm25_score\n",
        "    hybrid_results[text] = hybrid_score\n",
        "\n",
        "# Step 9: Re-ranking with Cross-Encoder\n",
        "rerank_inputs = [[query, text] for text in hybrid_results.keys()]\n",
        "rerank_scores = cross_encoder.predict(rerank_inputs)\n",
        "reranked_results = [\n",
        "    {\"text\": text, \"score\": rerank_scores[i]}\n",
        "    for i, text in enumerate(hybrid_results.keys())\n",
        "]\n",
        "reranked_results = sorted(reranked_results, key=lambda x: x[\"score\"], reverse=True)[:3]\n",
        "\n",
        "# Step 10: Abstractive Generation with Gemini\n",
        "context = \"\\n\".join([result[\"text\"] for result in reranked_results])\n",
        "prompt = f\"\"\"\n",
        "Query: {query}\n",
        "Context: {context}\n",
        "\n",
        "Summarize the key AI advancements in 2025 based on the provided context. Provide a concise, coherent response that captures the main points without quoting verbatim.\n",
        "Answer:\n",
        "\"\"\"\n",
        "model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
        "response = model.generate_content(prompt)\n",
        "summary = response.text\n",
        "\n",
        "# Step 11: Display Results\n",
        "print(\"\\nğŸ” Dense Retrieval Results for query:\", query)\n",
        "for hit in dense_hits:\n",
        "    print(f\"- {hit.payload['text']} (Score: {hit.score:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ” BM25 Sparse Retrieval Results for query:\", query)\n",
        "for result in bm25_results:\n",
        "    print(f\"- {result['text']} (Score: {result['score']:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ” Hybrid Retrieval Results for query:\", query)\n",
        "for text, score in sorted(hybrid_results.items(), key=lambda x: x[1], reverse=True)[:3]:\n",
        "    print(f\"- {text} (Hybrid Score: {score:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ” Re-ranked Hybrid Results for query:\", query)\n",
        "for result in reranked_results:\n",
        "    print(f\"- {result['text']} (Re-ranked Score: {result['score']:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ“ Summarized Answer:\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "uRXVbW0Qna8k",
        "outputId": "defd0d6c-f77d-4ed3-e65d-85b05dc40cad"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Collection 'Abstractive' created.\n",
            "âœ… Documents upserted to Qdrant for dense retrieval.\n",
            "\n",
            "ğŸ” Dense Retrieval Results for query: summarize AI advancements in 2025\n",
            "- In 2025, AI models achieved breakthroughs in multimodal processing, integrating text, images, and audio for applications in healthcare and autonomous vehicles. (Score: 0.8118)\n",
            "- Ethical AI frameworks gained traction in 2025, with new regulations in Europe to ensure transparency in AI decision-making. (Score: 0.7499)\n",
            "- Major tech companies in 2025 invested heavily in quantum AI, promising faster computation for complex problems like climate modeling. (Score: 0.7480)\n",
            "- AI-driven personalized education platforms expanded in 2025, tailoring curricula to individual student needs. (Score: 0.7437)\n",
            "\n",
            "ğŸ” BM25 Sparse Retrieval Results for query: summarize AI advancements in 2025\n",
            "- Major tech companies in 2025 invested heavily in quantum AI, promising faster computation for complex problems like climate modeling. (Score: 1.0718)\n",
            "- Ethical AI frameworks gained traction in 2025, with new regulations in Europe to ensure transparency in AI decision-making. (Score: 0.2978)\n",
            "- In 2025, AI models achieved breakthroughs in multimodal processing, integrating text, images, and audio for applications in healthcare and autonomous vehicles. (Score: 0.2858)\n",
            "- AI-driven personalized education platforms expanded in 2025, tailoring curricula to individual student needs. (Score: 0.2039)\n",
            "\n",
            "ğŸ” Hybrid Retrieval Results for query: summarize AI advancements in 2025\n",
            "- Major tech companies in 2025 invested heavily in quantum AI, promising faster computation for complex problems like climate modeling. (Hybrid Score: 0.9528)\n",
            "- In 2025, AI models achieved breakthroughs in multimodal processing, integrating text, images, and audio for applications in healthcare and autonomous vehicles. (Hybrid Score: 0.7067)\n",
            "- Ethical AI frameworks gained traction in 2025, with new regulations in Europe to ensure transparency in AI decision-making. (Hybrid Score: 0.6654)\n",
            "\n",
            "ğŸ” Re-ranked Hybrid Results for query: summarize AI advancements in 2025\n",
            "- In 2025, AI models achieved breakthroughs in multimodal processing, integrating text, images, and audio for applications in healthcare and autonomous vehicles. (Re-ranked Score: 4.7335)\n",
            "- Ethical AI frameworks gained traction in 2025, with new regulations in Europe to ensure transparency in AI decision-making. (Re-ranked Score: 3.4090)\n",
            "- AI-driven personalized education platforms expanded in 2025, tailoring curricula to individual student needs. (Re-ranked Score: 2.5640)\n",
            "\n",
            "ğŸ“ Summarized Answer:\n",
            "2025 saw significant AI progress in three key areas:  multimodal processing enabling advancements in healthcare and autonomous vehicles, increased focus and regulation around ethical AI and transparency (particularly in Europe), and wider adoption of personalized AI-powered education platforms.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mixed RAG**"
      ],
      "metadata": {
        "id": "FPml83qgn8w1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import CrossEncoder\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# ENV variables\n",
        "QDRANT_HOST = \"https://fd98050b-4928-44e1-9536-66478313e9c5.us-west-1-0.aws.cloud.qdrant.io\"\n",
        "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.EHI0nboatGy3QWLI6mf5tgpoGaLhSeNR9TtoetM16bE\"\n",
        "GEMINI_API_KEY = \"AIzaSyBLWAaYmdhyTHlXAnULNGVxuxx21sq_HBg\"\n",
        "\n",
        "# Initialize Gemini\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Initialize Qdrant client\n",
        "qdrant = QdrantClient(\n",
        "    url=QDRANT_HOST,\n",
        "    api_key=QDRANT_API_KEY,\n",
        ")\n",
        "\n",
        "# Initialize cross-encoder for re-ranking\n",
        "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "COLLECTION_NAME = \"mixed\"\n",
        "\n",
        "# Step 1: Create collection (if not exists)\n",
        "try:\n",
        "    qdrant.get_collection(collection_name=COLLECTION_NAME)\n",
        "    print(f\"âœ… Collection '{COLLECTION_NAME}' already exists.\")\n",
        "except Exception:\n",
        "    qdrant.create_collection(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
        "    )\n",
        "    # print(f\"âœ… Collection '{COLLECTION_NAME}' created.)\n",
        "\n",
        "# Step 2: Sample documents (AI risks context)\n",
        "docs = [\n",
        "    {\"id\": \"1\", \"text\": \"AI systems pose ethical risks, including bias in decision-making. 'Algorithms can perpetuate existing inequalities if not carefully designed,' warns a 2025 ethics report.\"},\n",
        "    {\"id\": \"2\", \"text\": \"Technical risks of AI include system failures and vulnerabilities to hacking. 'A single flaw in AI could lead to catastrophic consequences,' notes a cybersecurity expert.\"},\n",
        "    {\"id\": \"3\", \"text\": \"Societal risks from AI involve job displacement and privacy erosion. 'Automation may disrupt 30% of jobs by 2030,' predicts an economic study.\"},\n",
        "    {\"id\": \"4\", \"text\": \"Regulatory gaps in AI governance increase risks of misuse. 'Without global standards, AI could be weaponized,' states a policy brief.\"},\n",
        "]\n",
        "\n",
        "# Step 3: Embed documents & upload to Qdrant (Dense Retrieval)\n",
        "points = []\n",
        "for doc in docs:\n",
        "    response = genai.embed_content(model=\"models/embedding-001\", content=doc[\"text\"])\n",
        "    embedding = response[\"embedding\"]\n",
        "    points.append(PointStruct(id=int(doc[\"id\"]), vector=embedding, payload={\"text\": doc[\"text\"], \"doc_id\": doc[\"id\"]}))\n",
        "\n",
        "qdrant.upsert(collection_name=COLLECTION_NAME, points=points)\n",
        "print(\"âœ… Documents upserted to Qdrant for dense retrieval.\")\n",
        "\n",
        "# Step 4: BM25 Setup for Sparse Retrieval\n",
        "tokenized_docs = [doc[\"text\"].lower().split() for doc in docs]\n",
        "bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "# Step 5: Query\n",
        "query = \"summarize AI risks with quoted examples\"\n",
        "\n",
        "# Step 6: Dense Retrieval with Qdrant\n",
        "query_response = genai.embed_content(model=\"models/embedding-001\", content=query)\n",
        "query_vector = query_response[\"embedding\"]\n",
        "dense_hits = qdrant.query_points(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    query=query_vector,\n",
        "    limit=4,\n",
        "    with_payload=True\n",
        ").points\n",
        "\n",
        "# Step 7: BM25 Sparse Retrieval\n",
        "tokenized_query = query.lower().split()\n",
        "bm25_scores = bm25.get_scores(tokenized_query)\n",
        "bm25_results = [\n",
        "    {\"text\": docs[i][\"text\"], \"score\": bm25_scores[i], \"doc_id\": docs[i][\"id\"]}\n",
        "    for i in range(len(docs))\n",
        "    if bm25_scores[i] > 0\n",
        "]\n",
        "bm25_results = sorted(bm25_results, key=lambda x: x[\"score\"], reverse=True)[:4]\n",
        "\n",
        "# Step 8: Hybrid Retrieval\n",
        "dense_scores = {hit.payload[\"text\"]: hit.score for hit in dense_hits}\n",
        "bm25_scores = {result[\"text\"]: result[\"score\"] for result in bm25_results}\n",
        "all_texts = set(dense_scores.keys()).union(bm25_scores.keys())\n",
        "\n",
        "max_dense = max(dense_scores.values(), default=1.0)\n",
        "max_bm25 = max(bm25_scores.values(), default=1.0)\n",
        "\n",
        "hybrid_results = {}\n",
        "doc_ids = {hit.payload[\"text\"]: hit.payload[\"doc_id\"] for hit in dense_hits}  # Map text to doc_id\n",
        "for text in all_texts:\n",
        "    dense_score = dense_scores.get(text, 0) / max_dense\n",
        "    bm25_score = bm25_scores.get(text, 0) / max_bm25\n",
        "    hybrid_score = 0.6 * dense_score + 0.4 * bm25_score\n",
        "    hybrid_results[text] = {\"score\": hybrid_score, \"doc_id\": doc_ids.get(text, \"unknown\")}\n",
        "\n",
        "# Step 9: Re-ranking with Cross-Encoder\n",
        "rerank_inputs = [[query, text] for text in hybrid_results.keys()]\n",
        "rerank_scores = cross_encoder.predict(rerank_inputs)\n",
        "reranked_results = [\n",
        "    {\"text\": text, \"score\": rerank_scores[i], \"doc_id\": hybrid_results[text][\"doc_id\"]}\n",
        "    for i, text in enumerate(hybrid_results.keys())\n",
        "]\n",
        "reranked_results = sorted(reranked_results, key=lambda x: x[\"score\"], reverse=True)[:3]\n",
        "\n",
        "# Step 10: Snippet Extraction for Quotes\n",
        "quotes = []\n",
        "for result in reranked_results:\n",
        "    text = result[\"text\"]\n",
        "    doc_id = result[\"doc_id\"]\n",
        "    # Extract quoted text within single quotes\n",
        "    quoted_matches = re.findall(r\"'(.*?)'\", text)\n",
        "    for quote in quoted_matches:\n",
        "        if any(keyword in quote.lower() for keyword in [\"risk\", \"ai\", \"bias\", \"failure\", \"job\", \"privacy\", \"misuse\"]):\n",
        "            quotes.append({\"text\": quote, \"score\": result[\"score\"], \"doc_id\": doc_id})\n",
        "\n",
        "# Sort quotes by score and select top 2\n",
        "quotes = sorted(quotes, key=lambda x: x[\"score\"], reverse=True)[:2]\n",
        "\n",
        "# Step 11: Abstractive Summary with Gemini\n",
        "context = \"\\n\".join([result[\"text\"] for result in reranked_results])\n",
        "prompt = f\"\"\"\n",
        "Query: {query}\n",
        "Context: {context}\n",
        "\n",
        "Provide a concise summary of AI risks based on the context, integrating the following quoted examples with citations:\n",
        "{chr(10).join([f\"- '{q['text']}' (Document {q['doc_id']})\" for q in quotes])}\n",
        "\n",
        "The response should paraphrase the main points, include the quoted examples, and cite the document IDs in parentheses.\n",
        "Answer:\n",
        "\"\"\"\n",
        "model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
        "response = model.generate_content(prompt)\n",
        "summary = response.text\n",
        "\n",
        "# Step 12: Display Results\n",
        "print(\"\\nğŸ” Dense Retrieval Results for query:\", query)\n",
        "for hit in dense_hits:\n",
        "    print(f\"- {hit.payload['text']} (Score: {hit.score:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ” BM25 Sparse Retrieval Results for query:\", query)\n",
        "for result in bm25_results:\n",
        "    print(f\"- {result['text']} (Score: {result['score']:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ” Hybrid Retrieval Results for query:\", query)\n",
        "for text, info in sorted(hybrid_results.items(), key=lambda x: x[1][\"score\"], reverse=True)[:3]:\n",
        "    print(f\"- {text} (Hybrid Score: {info['score']:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ” Re-ranked Hybrid Results for query:\", query)\n",
        "for result in reranked_results:\n",
        "    print(f\"- {result['text']} (Re-ranked Score: {result['score']:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ“ Mixed RAG Answer:\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "4kjCQlWNn4y3",
        "outputId": "0c6cead2-ad7e-4c73-e93c-e3214f2a424d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Documents upserted to Qdrant for dense retrieval.\n",
            "\n",
            "ğŸ” Dense Retrieval Results for query: summarize AI risks with quoted examples\n",
            "- Technical risks of AI include system failures and vulnerabilities to hacking. 'A single flaw in AI could lead to catastrophic consequences,' notes a cybersecurity expert. (Score: 0.7800)\n",
            "- Societal risks from AI involve job displacement and privacy erosion. 'Automation may disrupt 30% of jobs by 2030,' predicts an economic study. (Score: 0.7466)\n",
            "- Regulatory gaps in AI governance increase risks of misuse. 'Without global standards, AI could be weaponized,' states a policy brief. (Score: 0.7223)\n",
            "- AI systems pose ethical risks, including bias in decision-making. 'Algorithms can perpetuate existing inequalities if not carefully designed,' warns a 2025 ethics report. (Score: 0.7053)\n",
            "\n",
            "ğŸ” BM25 Sparse Retrieval Results for query: summarize AI risks with quoted examples\n",
            "- Regulatory gaps in AI governance increase risks of misuse. 'Without global standards, AI could be weaponized,' states a policy brief. (Score: 0.4382)\n",
            "- Technical risks of AI include system failures and vulnerabilities to hacking. 'A single flaw in AI could lead to catastrophic consequences,' notes a cybersecurity expert. (Score: 0.4032)\n",
            "- Societal risks from AI involve job displacement and privacy erosion. 'Automation may disrupt 30% of jobs by 2030,' predicts an economic study. (Score: 0.3493)\n",
            "- AI systems pose ethical risks, including bias in decision-making. 'Algorithms can perpetuate existing inequalities if not carefully designed,' warns a 2025 ethics report. (Score: 0.1712)\n",
            "\n",
            "ğŸ” Hybrid Retrieval Results for query: summarize AI risks with quoted examples\n",
            "- Technical risks of AI include system failures and vulnerabilities to hacking. 'A single flaw in AI could lead to catastrophic consequences,' notes a cybersecurity expert. (Hybrid Score: 0.9680)\n",
            "- Regulatory gaps in AI governance increase risks of misuse. 'Without global standards, AI could be weaponized,' states a policy brief. (Hybrid Score: 0.9557)\n",
            "- Societal risks from AI involve job displacement and privacy erosion. 'Automation may disrupt 30% of jobs by 2030,' predicts an economic study. (Hybrid Score: 0.8932)\n",
            "\n",
            "ğŸ” Re-ranked Hybrid Results for query: summarize AI risks with quoted examples\n",
            "- Technical risks of AI include system failures and vulnerabilities to hacking. 'A single flaw in AI could lead to catastrophic consequences,' notes a cybersecurity expert. (Re-ranked Score: 3.3548)\n",
            "- Societal risks from AI involve job displacement and privacy erosion. 'Automation may disrupt 30% of jobs by 2030,' predicts an economic study. (Re-ranked Score: 1.5649)\n",
            "- AI systems pose ethical risks, including bias in decision-making. 'Algorithms can perpetuate existing inequalities if not carefully designed,' warns a 2025 ethics report. (Re-ranked Score: 0.5822)\n",
            "\n",
            "ğŸ“ Mixed RAG Answer:\n",
            "AI presents significant risks across technical, societal, and ethical domains.  Technical risks include system failures and vulnerabilities, with the potential for even a small error to have devastating effects: \"A single flaw in AI could lead to catastrophic consequences\" (Document 2). Societally, AI-driven automation threatens widespread job displacement, with one study projecting that \"Automation may disrupt 30% of jobs by 2030\" (Document 3).  Finally, ethical concerns arise from the potential for biased algorithms to perpetuate and exacerbate existing inequalities.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Agent-Based RAG**"
      ],
      "metadata": {
        "id": "8DwlvkBUpql-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import CrossEncoder\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "import json\n",
        "import re\n",
        "\n",
        "# ENV variables\n",
        "QDRANT_HOST = \"https://fd98050b-4928-44e1-9536-66478313e9c5.us-west-1-0.aws.cloud.qdrant.io\"\n",
        "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.EHI0nboatGy3QWLI6mf5tgpoGaLhSeNR9TtoetM16bE\"\n",
        "GEMINI_API_KEY = \"AIzaSyBLWAaYmdhyTHlXAnULNGVxuxx21sq_HBg\"\n",
        "\n",
        "# Initialize Gemini\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Initialize Qdrant client\n",
        "qdrant = QdrantClient(\n",
        "    url=QDRANT_HOST,\n",
        "    api_key=QDRANT_API_KEY,\n",
        ")\n",
        "\n",
        "# Initialize cross-encoder for re-ranking\n",
        "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "COLLECTION_NAME = \"Agent-Based\"\n",
        "\n",
        "# Step 1: Create collection (if not exists)\n",
        "try:\n",
        "    qdrant.get_collection(collection_name=COLLECTION_NAME)\n",
        "    print(f\"âœ… Collection '{COLLECTION_NAME}' already exists.\")\n",
        "except Exception:\n",
        "    qdrant.create_collection(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
        "    )\n",
        "    print(f\"âœ… Collection '{COLLECTION_NAME}' created.\")\n",
        "\n",
        "# Step 2: Sample local documents (vertical farming developments)\n",
        "docs = [\n",
        "    {\"id\": \"1\", \"text\": \"In 2024, AeroFarms expanded its vertical farming operations with a new facility in Saudi Arabia, leveraging AI-driven analytics for crop optimization.\"},\n",
        "    {\"id\": \"2\", \"text\": \"LED lighting advancements in 2023 reduced energy costs for vertical farms by 20%, enabling wider adoption of hydroponics.\"},\n",
        "    {\"id\": \"3\", \"text\": \"Urban Crop Solutions launched a financing program with Siemens in 2022 to support scalable vertical farming infrastructure.\"},\n",
        "    {\"id\": \"4\", \"text\": \"The global vertical farming market grew to USD 7.51 billion in 2024, driven by demand for organic produce and sustainable practices.\"},\n",
        "]\n",
        "\n",
        "# Step 3: Embed documents & upload to Qdrant (Dense Retrieval)\n",
        "points = []\n",
        "for doc in docs:\n",
        "    response = genai.embed_content(model=\"models/embedding-001\", content=doc[\"text\"])\n",
        "    embedding = response[\"embedding\"]\n",
        "    points.append(PointStruct(id=int(doc[\"id\"]), vector=embedding, payload={\"text\": doc[\"text\"], \"doc_id\": doc[\"id\"]}))\n",
        "\n",
        "qdrant.upsert(collection_name=COLLECTION_NAME, points=points)\n",
        "print(\"âœ… Documents upserted to Qdrant for dense retrieval.\")\n",
        "\n",
        "# Step 4: BM25 Setup for Sparse Retrieval\n",
        "tokenized_docs = [doc[\"text\"].lower().split() for doc in docs]\n",
        "bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "# Step 5: Simulated Web Search Tool (using provided web results)\n",
        "def web_search_tool(query: str) -> List[Dict]:\n",
        "    web_results = [\n",
        "        {\n",
        "            \"source\": \"grandviewresearch.com\",\n",
        "            \"text\": \"The global vertical farming market size was valued at USD 6.92 billion in 2023 and is expected to grow at a CAGR of 20.1% from 2023 to 2030. Vertical farms are becoming technologically advanced, with the use of LED lights and automated control systems.\",\n",
        "            \"url\": \"https://www.grandviewresearch.com\"\n",
        "        },\n",
        "        {\n",
        "            \"source\": \"marketsandmarkets.com\",\n",
        "            \"text\": \"The global vertical farming market size was estimated at USD 5.6 billion in 2024 and is poised to reach USD 13.7 billion by 2029, growing at a CAGR of 19.7%. Developments in IoT, AI, and hydroponics increase efficiency.\",\n",
        "            \"url\": \"https://www.marketsandmarkets.com\"\n",
        "        },\n",
        "        {\n",
        "            \"source\": \"straitsresearch.com\",\n",
        "            \"text\": \"In urban settings, vertical farms develop a farm-to-table system, reducing food packaging and waste. LED technology advancements drive market growth.\",\n",
        "            \"url\": \"https://straitsresearch.com\"\n",
        "        }\n",
        "    ]\n",
        "    return web_results\n",
        "\n",
        "# Step 6: Gemini-Powered Agent\n",
        "class GeminiAgent:\n",
        "    def __init__(self, model_name: str = \"gemini-1.5-pro\"):\n",
        "        self.model = genai.GenerativeModel(model_name)\n",
        "\n",
        "    def plan_retrieval(self, query: str) -> Dict:\n",
        "        prompt = f\"\"\"\n",
        "        Query: {query}\n",
        "        You are an agent planning data retrieval for a market report. Decide which tools to use:\n",
        "        - Web search for real-time market data and trends\n",
        "        - Local document search for internal data\n",
        "        Provide a plan as a JSON object with 'tools' (list) and 'rationale' (string). Ensure the response is valid JSON without markdown or code blocks.\n",
        "        Example:\n",
        "        {{\"tools\": [\"web_search\", \"local_search\"], \"rationale\": \"Web search for real-time data and local search for internal insights.\"}}\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            # Clean response: remove markdown code blocks or other formatting\n",
        "            cleaned_text = re.sub(r'```(?:json)?\\n|\\n```', '', response.text).strip()\n",
        "            # Parse JSON safely\n",
        "            plan = json.loads(cleaned_text)\n",
        "            # Validate expected structure\n",
        "            if not isinstance(plan, dict) or \"tools\" not in plan or \"rationale\" not in plan:\n",
        "                raise ValueError(\"Invalid plan structure\")\n",
        "            return plan\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error parsing plan: {e}\")\n",
        "            print(f\"Raw response: {response.text}\")\n",
        "            # Fallback plan\n",
        "            return {\n",
        "                \"tools\": [\"web_search\", \"local_search\"],\n",
        "                \"rationale\": \"Fallback: Use web search for real-time data and local search for internal insights due to parsing error.\"\n",
        "            }\n",
        "\n",
        "    def execute_retrieval(self, plan: Dict, query: str) -> List[Dict]:\n",
        "        results = []\n",
        "        for tool in plan[\"tools\"]:\n",
        "            if tool == \"web_search\":\n",
        "                web_results = web_search_tool(query)\n",
        "                results.extend([\n",
        "                    {\"text\": r[\"text\"], \"source\": r[\"source\"], \"url\": r[\"url\"], \"type\": \"web\"}\n",
        "                    for r in web_results\n",
        "                ])\n",
        "            elif tool == \"local_search\":\n",
        "                # Dense Retrieval\n",
        "                query_response = genai.embed_content(model=\"models/embedding-001\", content=query)\n",
        "                query_vector = query_response[\"embedding\"]\n",
        "                dense_hits = qdrant.query_points(\n",
        "                    collection_name=COLLECTION_NAME,\n",
        "                    query=query_vector,\n",
        "                    limit=4,\n",
        "                    with_payload=True\n",
        "                ).points\n",
        "                results.extend([\n",
        "                    {\"text\": hit.payload[\"text\"], \"source\": f\"Local Doc {hit.payload['doc_id']}\", \"type\": \"local\"}\n",
        "                    for hit in dense_hits\n",
        "                ])\n",
        "                # Sparse Retrieval (BM25)\n",
        "                tokenized_query = query.lower().split()\n",
        "                bm25_scores = bm25.get_scores(tokenized_query)\n",
        "                bm25_results = [\n",
        "                    {\"text\": docs[i][\"text\"], \"score\": bm25_scores[i], \"doc_id\": docs[i][\"id\"]}\n",
        "                    for i in range(len(docs))\n",
        "                    if bm25_scores[i] > 0\n",
        "                ]\n",
        "                bm25_results = sorted(bm25_results, key=lambda x: x[\"score\"], reverse=True)[:4]\n",
        "                results.extend([\n",
        "                    {\"text\": r[\"text\"], \"source\": f\"Local Doc {r['doc_id']}\", \"type\": \"local\"}\n",
        "                    for r in bm25_results\n",
        "                ])\n",
        "        return results\n",
        "\n",
        "    def generate_report(self, query: str, retrieved_data: List[Dict]) -> str:\n",
        "        context = \"\\n\".join([f\"Source: {d['source']}\\n{d['text']}\" for d in retrieved_data])\n",
        "        prompt = f\"\"\"\n",
        "        Query: {query}\n",
        "        Context: {context}\n",
        "\n",
        "        Generate a concise market report on recent developments in vertical farming. Include:\n",
        "        - A summary of market size and growth trends.\n",
        "        - Key technological advancements.\n",
        "        - Notable industry developments (e.g., partnerships, expansions).\n",
        "        - Citations for sources in parentheses (e.g., grandviewresearch.com).\n",
        "        The response should be coherent, paraphrased, and professional, avoiding verbatim quotes unless necessary.\n",
        "        Answer:\n",
        "        \"\"\"\n",
        "        response = self.model.generate_content(prompt)\n",
        "        return response.text\n",
        "\n",
        "# Step 7: Query\n",
        "query = \"market report on vertical farming recent developments\"\n",
        "\n",
        "# Step 8: Agent Execution\n",
        "agent = GeminiAgent()\n",
        "plan = agent.plan_retrieval(query)\n",
        "print(\"\\nğŸ“‹ Retrieval Plan:\", plan)\n",
        "\n",
        "retrieved_data = agent.execute_retrieval(plan, query)\n",
        "\n",
        "# Step 9: Re-ranking with Cross-Encoder\n",
        "rerank_inputs = [[query, data[\"text\"]] for data in retrieved_data]\n",
        "rerank_scores = cross_encoder.predict(rerank_inputs)\n",
        "reranked_data = [\n",
        "    {\"text\": data[\"text\"], \"source\": data[\"source\"], \"score\": rerank_scores[i], \"type\": data[\"type\"]}\n",
        "    for i, data in enumerate(retrieved_data)\n",
        "]\n",
        "reranked_data = sorted(reranked_data, key=lambda x: x[\"score\"], reverse=True)[:5]\n",
        "\n",
        "# Step 10: Generate Market Report\n",
        "report = agent.generate_report(query, reranked_data)\n",
        "\n",
        "# Step 11: Display Results\n",
        "print(\"\\nğŸ” Retrieved Data:\")\n",
        "for data in retrieved_data:\n",
        "    print(f\"- Source: {data['source']}\\n  {data['text']}\")\n",
        "\n",
        "print(\"\\nğŸ” Re-ranked Data:\")\n",
        "for data in reranked_data:\n",
        "    print(f\"- Source: {data['source']} (Score: {data['score']:.4f})\\n  {data['text']}\")\n",
        "\n",
        "print(\"\\nğŸ“ Market Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "4yYkA9HaoqLo",
        "outputId": "3751b1cc-9b53-4031-cbd0-5cf919115a97"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Collection 'Agent-Based' created.\n",
            "âœ… Documents upserted to Qdrant for dense retrieval.\n",
            "\n",
            "ğŸ“‹ Retrieval Plan: {'tools': ['web_search', 'local_document_search'], 'rationale': 'Use web search to gather recent developments, market trends, competitor analysis, and publicly available market data on vertical farming. Utilize local document search to find internal reports, presentations, and data that may offer proprietary insights and complement publicly available information for a comprehensive market report.'}\n",
            "\n",
            "ğŸ” Retrieved Data:\n",
            "- Source: grandviewresearch.com\n",
            "  The global vertical farming market size was valued at USD 6.92 billion in 2023 and is expected to grow at a CAGR of 20.1% from 2023 to 2030. Vertical farms are becoming technologically advanced, with the use of LED lights and automated control systems.\n",
            "- Source: marketsandmarkets.com\n",
            "  The global vertical farming market size was estimated at USD 5.6 billion in 2024 and is poised to reach USD 13.7 billion by 2029, growing at a CAGR of 19.7%. Developments in IoT, AI, and hydroponics increase efficiency.\n",
            "- Source: straitsresearch.com\n",
            "  In urban settings, vertical farms develop a farm-to-table system, reducing food packaging and waste. LED technology advancements drive market growth.\n",
            "\n",
            "ğŸ” Re-ranked Data:\n",
            "- Source: marketsandmarkets.com (Score: 6.2352)\n",
            "  The global vertical farming market size was estimated at USD 5.6 billion in 2024 and is poised to reach USD 13.7 billion by 2029, growing at a CAGR of 19.7%. Developments in IoT, AI, and hydroponics increase efficiency.\n",
            "- Source: grandviewresearch.com (Score: 5.4615)\n",
            "  The global vertical farming market size was valued at USD 6.92 billion in 2023 and is expected to grow at a CAGR of 20.1% from 2023 to 2030. Vertical farms are becoming technologically advanced, with the use of LED lights and automated control systems.\n",
            "- Source: straitsresearch.com (Score: 2.4799)\n",
            "  In urban settings, vertical farms develop a farm-to-table system, reducing food packaging and waste. LED technology advancements drive market growth.\n",
            "\n",
            "ğŸ“ Market Report:\n",
            "Vertical Farming Market: Recent Developments\n",
            "\n",
            "The vertical farming market is experiencing rapid growth, driven by technological advancements and the increasing demand for sustainable agriculture.  Market size estimates vary slightly across research firms, but all indicate robust expansion. MarketsandMarkets projects a market value of USD 5.6 billion in 2024, reaching USD 13.7 billion by 2029 (marketsandmarkets.com). Grand View Research estimates a slightly higher value of USD 6.92 billion in 2023, with a projected CAGR of 20.1% through 2030 (grandviewresearch.com).  These figures suggest a strong upward trajectory for the sector.\n",
            "\n",
            "Key technological advancements are fueling this growth.  The integration of Internet of Things (IoT), artificial intelligence (AI), and advanced hydroponic systems is improving efficiency and yields (marketsandmarkets.com).  The use of LED lighting and automated control systems is further enhancing productivity and resource optimization within vertical farms (grandviewresearch.com). Continued advancements in LED technology are a significant driver of market expansion (straitsresearch.com).\n",
            "\n",
            "Beyond technology, the industry is witnessing developments focused on sustainability and localized food systems.  Vertical farms are playing a growing role in urban agriculture, shortening the farm-to-table distance and reducing food packaging and waste (straitsresearch.com). While specific examples of partnerships and expansions were not detailed within the provided source material, the overall trend points towards increased investment and adoption of vertical farming practices.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multi-Modal RAG**"
      ],
      "metadata": {
        "id": "OOUXh83NrcK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import CrossEncoder\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "\n",
        "# ENV variables\n",
        "QDRANT_HOST = \"https://fd98050b-4928-44e1-9536-66478313e9c5.us-west-1-0.aws.cloud.qdrant.io\"\n",
        "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.EHI0nboatGy3QWLI6mf5tgpoGaLhSeNR9TtoetM16bE\"\n",
        "GEMINI_API_KEY = \"AIzaSyBLWAaYmdhyTHlXAnULNGVxuxx21sq_HBg\"\n",
        "\n",
        "# Initialize Gemini\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Initialize Qdrant client\n",
        "qdrant = QdrantClient(\n",
        "    url=QDRANT_HOST,\n",
        "    api_key=QDRANT_API_KEY,\n",
        ")\n",
        "\n",
        "# Initialize cross-encoder for re-ranking\n",
        "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "COLLECTION_NAME = \"Multi-Modal\"\n",
        "\n",
        "# Step 1: Create collection (if not exists)\n",
        "try:\n",
        "    qdrant.get_collection(collection_name=COLLECTION_NAME)\n",
        "    print(f\"âœ… Collection '{COLLECTION_NAME}' already exists.\")\n",
        "except Exception:\n",
        "    qdrant.create_collection(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
        "    )\n",
        "    print(f\"âœ… Collection '{COLLECTION_NAME}' created.\")\n",
        "\n",
        "# Step 2: Sample documents (bird descriptions)\n",
        "docs = [\n",
        "    {\"id\": \"1\", \"text\": \"The Northern Cardinal is a medium-sized songbird with a bright red crest, red body, and black face mask. It is commonly found in North America.\"},\n",
        "    {\"id\": \"2\", \"text\": \"The American Robin is a migratory bird with a reddish-orange breast, dark wings, and a white eye ring. It is widespread across the United States.\"},\n",
        "    {\"id\": \"3\", \"text\": \"The Blue Jay is known for its striking blue and white plumage, with a distinctive crest and black collar. It inhabits woodlands and suburban areas.\"},\n",
        "    {\"id\": \"4\", \"text\": \"The Black-capped Chickadee is a small bird with a black cap, white cheeks, and gray wings. It is known for its cheerful 'chick-a-dee' call.\"},\n",
        "]\n",
        "\n",
        "# Step 3: Embed documents & upload to Qdrant (Dense Retrieval)\n",
        "points = []\n",
        "for doc in docs:\n",
        "    response = genai.embed_content(model=\"models/embedding-001\", content=doc[\"text\"])\n",
        "    embedding = response[\"embedding\"]\n",
        "    points.append(PointStruct(id=int(doc[\"id\"]), vector=embedding, payload={\"text\": doc[\"text\"], \"doc_id\": doc[\"id\"]}))\n",
        "\n",
        "qdrant.upsert(collection_name=COLLECTION_NAME, points=points)\n",
        "print(\"âœ… Documents upserted to Qdrant for dense retrieval.\")\n",
        "\n",
        "# Step 4: BM25 Setup for Sparse Retrieval\n",
        "tokenized_docs = [doc[\"text\"].lower().split() for doc in docs]\n",
        "bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "# Step 5: Simulated Image Input and Query\n",
        "# In practice, replace image_description with an actual image file (e.g., Image.open(\"bird.jpg\"))\n",
        "image_description = \"A small bird with a red crest and black wings.\"\n",
        "text_query = \"What bird is this?\"\n",
        "combined_query = f\"{text_query} Description: {image_description}\"\n",
        "\n",
        "# Step 6: Simulated Visual Database (for image comparison)\n",
        "# In practice, store image embeddings in Qdrant using a vision model (e.g., CLIP)\n",
        "visual_db = [\n",
        "    {\"id\": \"img1\", \"description\": \"Bright red crest, black face, red body\", \"species\": \"Northern Cardinal\"},\n",
        "    {\"id\": \"img2\", \"description\": \"Reddish-orange breast, dark wings\", \"species\": \"American Robin\"},\n",
        "    {\"id\": \"img3\", \"description\": \"Blue and white plumage, black collar\", \"species\": \"Blue Jay\"},\n",
        "    {\"id\": \"img4\", \"description\": \"Black cap, white cheeks, gray wings\", \"species\": \"Black-capped Chickadee\"},\n",
        "]\n",
        "\n",
        "def match_image(image_description: str) -> list[dict]:\n",
        "    # Simulate image matching by comparing descriptions\n",
        "    matches = []\n",
        "    for img in visual_db:\n",
        "        if \"red crest\" in image_description.lower() and \"red crest\" in img[\"description\"].lower():\n",
        "            matches.append({\"species\": img[\"species\"], \"description\": img[\"description\"], \"score\": 0.9})\n",
        "        elif \"black wings\" in image_description.lower() and \"wings\" in img[\"description\"].lower():\n",
        "            matches.append({\"species\": img[\"species\"], \"description\": img[\"description\"], \"score\": 0.7})\n",
        "    return sorted(matches, key=lambda x: x[\"score\"], reverse=True)[:1]\n",
        "\n",
        "# Step 7: Dense Retrieval with Qdrant\n",
        "query_response = genai.embed_content(model=\"models/embedding-001\", content=combined_query)\n",
        "query_vector = query_response[\"embedding\"]\n",
        "dense_hits = qdrant.query_points(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    query=query_vector,\n",
        "    limit=4,\n",
        "    with_payload=True\n",
        ").points\n",
        "\n",
        "# Step 8: BM25 Sparse Retrieval\n",
        "tokenized_query = combined_query.lower().split()\n",
        "bm25_scores = bm25.get_scores(tokenized_query)\n",
        "bm25_results = [\n",
        "    {\"text\": docs[i][\"text\"], \"score\": bm25_scores[i], \"doc_id\": docs[i][\"id\"]}\n",
        "    for i in range(len(docs))\n",
        "    if bm25_scores[i] > 0\n",
        "]\n",
        "bm25_results = sorted(bm25_results, key=lambda x: x[\"score\"], reverse=True)[:4]\n",
        "\n",
        "# Step 9: Hybrid Retrieval\n",
        "dense_scores = {hit.payload[\"text\"]: hit.score for hit in dense_hits}\n",
        "bm25_scores = {result[\"text\"]: result[\"score\"] for result in bm25_results}\n",
        "all_texts = set(dense_scores.keys()).union(bm25_scores.keys())\n",
        "\n",
        "max_dense = max(dense_scores.values(), default=1.0)\n",
        "max_bm25 = max(bm25_scores.values(), default=1.0)\n",
        "\n",
        "hybrid_results = {}\n",
        "doc_ids = {hit.payload[\"text\"]: hit.payload[\"doc_id\"] for hit in dense_hits}\n",
        "for text in all_texts:\n",
        "    dense_score = dense_scores.get(text, 0) / max_dense\n",
        "    bm25_score = bm25_scores.get(text, 0) / max_bm25\n",
        "    hybrid_score = 0.6 * dense_score + 0.4 * bm25_score\n",
        "    hybrid_results[text] = {\"score\": hybrid_score, \"doc_id\": doc_ids.get(text, \"unknown\")}\n",
        "\n",
        "# Step 10: Re-ranking with Cross-Encoder\n",
        "rerank_inputs = [[combined_query, text] for text in hybrid_results.keys()]\n",
        "rerank_scores = cross_encoder.predict(rerank_inputs)\n",
        "reranked_results = [\n",
        "    {\"text\": text, \"score\": rerank_scores[i], \"doc_id\": hybrid_results[text][\"doc_id\"]}\n",
        "    for i, text in enumerate(hybrid_results.keys())\n",
        "]\n",
        "reranked_results = sorted(reranked_results, key=lambda x: x[\"score\"], reverse=True)[:3]\n",
        "\n",
        "# Step 11: Image Matching\n",
        "image_matches = match_image(image_description)\n",
        "image_context = \"\\n\".join([f\"Species: {m['species']}, Description: {m['description']}\" for m in image_matches])\n",
        "\n",
        "# Step 12: Multi-Modal Generation with Gemini\n",
        "context = \"\\n\".join([result[\"text\"] for result in reranked_results])\n",
        "prompt = f\"\"\"\n",
        "Query: {text_query}\n",
        "Image Description: {image_description}\n",
        "Textual Context: {context}\n",
        "Image Context: {image_context}\n",
        "\n",
        "Identify the bird based on the image description and provided context. Provide a concise response, including:\n",
        "- The bird species.\n",
        "- Key identifying features.\n",
        "- A brief description from the context to support the identification.\n",
        "Use the textual and image context to ensure accuracy. Return the response as plain text without markdown or code blocks.\n",
        "\"\"\"\n",
        "model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
        "# In practice, pass an actual image: response = model.generate_content([prompt, Image.open(\"bird.jpg\")])\n",
        "response = model.generate_content(prompt)\n",
        "# Clean response to remove any markdown\n",
        "cleaned_response = re.sub(r'```(?:text)?\\n|\\n```', '', response.text).strip()\n",
        "identification = cleaned_response\n",
        "\n",
        "# Step 13: Display Results\n",
        "print(\"\\nğŸ” Dense Retrieval Results for query:\", combined_query)\n",
        "for hit in dense_hits:\n",
        "    print(f\"- {hit.payload['text']} (Score: {hit.score:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ” BM25 Sparse Retrieval Results for query:\", combined_query)\n",
        "for result in bm25_results:\n",
        "    print(f\"- {result['text']} (Score: {result['score']:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ” Hybrid Retrieval Results for query:\", combined_query)\n",
        "for text, info in sorted(hybrid_results.items(), key=lambda x: x[1][\"score\"], reverse=True)[:3]:\n",
        "    print(f\"- {text} (Hybrid Score: {info['score']:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ” Re-ranked Hybrid Results for query:\", combined_query)\n",
        "for result in reranked_results:\n",
        "    print(f\"- {result['text']} (Re-ranked Score: {result['score']:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ” Image Matching Results:\")\n",
        "for match in image_matches:\n",
        "    print(f\"- Species: {match['species']} (Score: {match['score']:.4f})\")\n",
        "\n",
        "print(\"\\nğŸ“ Bird Identification:\")\n",
        "print(identification)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "rSLiXwrtp4y-",
        "outputId": "006392b8-95cb-4031-cc38-f0ee44b6a924"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Collection 'Multi-Modal' created.\n",
            "âœ… Documents upserted to Qdrant for dense retrieval.\n",
            "\n",
            "ğŸ” Dense Retrieval Results for query: What bird is this? Description: A small bird with a red crest and black wings.\n",
            "- The Northern Cardinal is a medium-sized songbird with a bright red crest, red body, and black face mask. It is commonly found in North America. (Score: 0.7643)\n",
            "- The Black-capped Chickadee is a small bird with a black cap, white cheeks, and gray wings. It is known for its cheerful 'chick-a-dee' call. (Score: 0.7496)\n",
            "- The American Robin is a migratory bird with a reddish-orange breast, dark wings, and a white eye ring. It is widespread across the United States. (Score: 0.7245)\n",
            "- The Blue Jay is known for its striking blue and white plumage, with a distinctive crest and black collar. It inhabits woodlands and suburban areas. (Score: 0.7099)\n",
            "\n",
            "ğŸ” BM25 Sparse Retrieval Results for query: What bird is this? Description: A small bird with a red crest and black wings.\n",
            "- The Black-capped Chickadee is a small bird with a black cap, white cheeks, and gray wings. It is known for its cheerful 'chick-a-dee' call. (Score: 2.5226)\n",
            "- The Northern Cardinal is a medium-sized songbird with a bright red crest, red body, and black face mask. It is commonly found in North America. (Score: 1.9990)\n",
            "- The Blue Jay is known for its striking blue and white plumage, with a distinctive crest and black collar. It inhabits woodlands and suburban areas. (Score: 1.5684)\n",
            "- The American Robin is a migratory bird with a reddish-orange breast, dark wings, and a white eye ring. It is widespread across the United States. (Score: 0.7358)\n",
            "\n",
            "ğŸ” Hybrid Retrieval Results for query: What bird is this? Description: A small bird with a red crest and black wings.\n",
            "- The Black-capped Chickadee is a small bird with a black cap, white cheeks, and gray wings. It is known for its cheerful 'chick-a-dee' call. (Hybrid Score: 0.9884)\n",
            "- The Northern Cardinal is a medium-sized songbird with a bright red crest, red body, and black face mask. It is commonly found in North America. (Hybrid Score: 0.9170)\n",
            "- The Blue Jay is known for its striking blue and white plumage, with a distinctive crest and black collar. It inhabits woodlands and suburban areas. (Hybrid Score: 0.8059)\n",
            "\n",
            "ğŸ” Re-ranked Hybrid Results for query: What bird is this? Description: A small bird with a red crest and black wings.\n",
            "- The Black-capped Chickadee is a small bird with a black cap, white cheeks, and gray wings. It is known for its cheerful 'chick-a-dee' call. (Re-ranked Score: 5.6333)\n",
            "- The Northern Cardinal is a medium-sized songbird with a bright red crest, red body, and black face mask. It is commonly found in North America. (Re-ranked Score: 5.0800)\n",
            "- The American Robin is a migratory bird with a reddish-orange breast, dark wings, and a white eye ring. It is widespread across the United States. (Re-ranked Score: 1.6856)\n",
            "\n",
            "ğŸ” Image Matching Results:\n",
            "- Species: Northern Cardinal (Score: 0.9000)\n",
            "\n",
            "ğŸ“ Bird Identification:\n",
            "Northern Cardinal. Red crest, black wings (partially black face mask). \"A medium-sized songbird with a bright red crest, red body, and black face mask.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Memory-Augmented RAG**"
      ],
      "metadata": {
        "id": "p_UUrlJgsT3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import CrossEncoder\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import List, Dict\n",
        "\n",
        "# ENV variables\n",
        "QDRANT_HOST = \"https://fd98050b-4928-44e1-9536-66478313e9c5.us-west-1-0.aws.cloud.qdrant.io\"\n",
        "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.EHI0nboatGy3QWLI6mf5tgpoGaLhSeNR9TtoetM16bE\"\n",
        "GEMINI_API_KEY = \"AIzaSyBLWAaYmdhyTHlXAnULNGVxuxx21sq_HBg\"\n",
        "\n",
        "# Initialize Gemini\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Initialize Qdrant client\n",
        "qdrant = QdrantClient(\n",
        "    url=QDRANT_HOST,\n",
        "    api_key=QDRANT_API_KEY,\n",
        ")\n",
        "\n",
        "# Initialize cross-encoder for re-ranking\n",
        "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "COLLECTION_NAME = \"Memory-Augmented\"\n",
        "\n",
        "# Step 1: Create collection (if not exists)\n",
        "try:\n",
        "    qdrant.get_collection(collection_name=COLLECTION_NAME)\n",
        "    print(f\"âœ… Collection '{COLLECTION_NAME}' already exists.\")\n",
        "except Exception:\n",
        "    qdrant.create_collection(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
        "    )\n",
        "    print(f\"âœ… Collection '{COLLECTION_NAME}' created.\")\n",
        "\n",
        "# Step 2: Sample documents (router troubleshooting)\n",
        "docs = [\n",
        "    {\"id\": \"1\", \"text\": \"If the router is not connecting to the internet after a reboot, check the Ethernet cable connection to the modem and ensure the modem is powered on.\"},\n",
        "    {\"id\": \"2\", \"text\": \"A common issue post-reboot is incorrect Wi-Fi settings. Verify the SSID and password in the routerâ€™s admin panel.\"},\n",
        "    {\"id\": \"3\", \"text\": \"If the routerâ€™s lights are blinking amber after reboot, perform a factory reset by holding the reset button for 10 seconds.\"},\n",
        "    {\"id\": \"4\", \"text\": \"Slow internet after rebooting the router may indicate interference. Change the Wi-Fi channel to 1, 6, or 11 in the router settings.\"},\n",
        "]\n",
        "\n",
        "# Step 3: Embed documents & upload to Qdrant (Dense Retrieval)\n",
        "points = []\n",
        "for doc in docs:\n",
        "    response = genai.embed_content(model=\"models/embedding-001\", content=doc[\"text\"])\n",
        "    embedding = response[\"embedding\"]\n",
        "    points.append(PointStruct(id=int(doc[\"id\"]), vector=embedding, payload={\"text\": doc[\"text\"], \"doc_id\": doc[\"id\"]}))\n",
        "\n",
        "qdrant.upsert(collection_name=COLLECTION_NAME, points=points)\n",
        "print(\"âœ… Documents upserted to Qdrant for dense retrieval.\")\n",
        "\n",
        "# Step 4: BM25 Setup for Sparse Retrieval\n",
        "tokenized_docs = [doc[\"text\"].lower().split() for doc in docs]\n",
        "bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "# Step 5: Memory-Augmented Chatbot Class\n",
        "class TroubleshootingChatbot:\n",
        "    def __init__(self, model_name: str = \"gemini-1.5-pro\", memory_size: int = 5):\n",
        "        self.model = genai.GenerativeModel(model_name)\n",
        "        self.memory: List[Dict] = []  # Store conversation history\n",
        "        self.memory_size = memory_size  # Limit memory to last N exchanges\n",
        "\n",
        "    def add_to_memory(self, user_query: str, system_response: str):\n",
        "        \"\"\"Add a user query and system response to memory.\"\"\"\n",
        "        self.memory.append({\"user\": user_query, \"system\": system_response})\n",
        "        # Keep only the last memory_size exchanges\n",
        "        self.memory = self.memory[-self.memory_size:]\n",
        "\n",
        "    def get_context(self) -> str:\n",
        "        \"\"\"Generate context string from memory.\"\"\"\n",
        "        context = \"\"\n",
        "        for exchange in self.memory:\n",
        "            context += f\"User: {exchange['user']}\\nSystem: {exchange['system']}\\n\"\n",
        "        return context.strip()\n",
        "\n",
        "    def retrieve_documents(self, query: str) -> List[Dict]:\n",
        "        \"\"\"Retrieve documents using hybrid retrieval, augmented by conversation context.\"\"\"\n",
        "        # Combine current query with memory context\n",
        "        context = self.get_context()\n",
        "        augmented_query = f\"{context}\\nCurrent Query: {query}\" if context else query\n",
        "\n",
        "        # Dense Retrieval\n",
        "        query_response = genai.embed_content(model=\"models/embedding-001\", content=augmented_query)\n",
        "        query_vector = query_response[\"embedding\"]\n",
        "        dense_hits = qdrant.query_points(\n",
        "            collection_name=COLLECTION_NAME,\n",
        "            query=query_vector,\n",
        "            limit=4,\n",
        "            with_payload=True\n",
        "        ).points\n",
        "\n",
        "        # Sparse Retrieval (BM25)\n",
        "        tokenized_query = augmented_query.lower().split()\n",
        "        bm25_scores = bm25.get_scores(tokenized_query)\n",
        "        bm25_results = [\n",
        "            {\"text\": docs[i][\"text\"], \"score\": bm25_scores[i], \"doc_id\": docs[i][\"id\"]}\n",
        "            for i in range(len(docs))\n",
        "            if bm25_scores[i] > 0\n",
        "        ]\n",
        "        bm25_results = sorted(bm25_results, key=lambda x: x[\"score\"], reverse=True)[:4]\n",
        "\n",
        "        # Hybrid Retrieval\n",
        "        dense_scores = {hit.payload[\"text\"]: hit.score for hit in dense_hits}\n",
        "        bm25_scores = {result[\"text\"]: result[\"score\"] for result in bm25_results}\n",
        "        all_texts = set(dense_scores.keys()).union(bm25_scores.keys())\n",
        "\n",
        "        max_dense = max(dense_scores.values(), default=1.0)\n",
        "        max_bm25 = max(bm25_scores.values(), default=1.0)\n",
        "\n",
        "        hybrid_results = {}\n",
        "        doc_ids = {hit.payload[\"text\"]: hit.payload[\"doc_id\"] for hit in dense_hits}\n",
        "        for text in all_texts:\n",
        "            dense_score = dense_scores.get(text, 0) / max_dense\n",
        "            bm25_score = bm25_scores.get(text, 0) / max_bm25\n",
        "            hybrid_score = 0.6 * dense_score + 0.4 * bm25_score\n",
        "            hybrid_results[text] = {\"score\": hybrid_score, \"doc_id\": doc_ids.get(text, \"unknown\")}\n",
        "\n",
        "        # Re-ranking with Cross-Encoder\n",
        "        rerank_inputs = [[augmented_query, text] for text in hybrid_results.keys()]\n",
        "        rerank_scores = cross_encoder.predict(rerank_inputs)\n",
        "        reranked_results = [\n",
        "            {\"text\": text, \"score\": rerank_scores[i], \"doc_id\": hybrid_results[text][\"doc_id\"]}\n",
        "            for i, text in enumerate(hybrid_results.keys())\n",
        "        ]\n",
        "        return sorted(reranked_results, key=lambda x: x[\"score\"], reverse=True)[:3]\n",
        "\n",
        "    def generate_response(self, query: str, retrieved_docs: List[Dict]) -> str:\n",
        "        \"\"\"Generate a troubleshooting response using conversation history and retrieved documents.\"\"\"\n",
        "        context = self.get_context()\n",
        "        doc_context = \"\\n\".join([f\"Doc {d['doc_id']}: {d['text']}\" for d in retrieved_docs])\n",
        "        prompt = f\"\"\"\n",
        "        You are a troubleshooting chatbot helping with router issues. Use the conversation history and retrieved documents to provide a concise, relevant response to the current query. Avoid markdown or code blocks in the response.\n",
        "\n",
        "        Conversation History:\n",
        "        {context}\n",
        "\n",
        "        Retrieved Documents:\n",
        "        {doc_context}\n",
        "\n",
        "        Current Query: {query}\n",
        "\n",
        "        Provide a clear troubleshooting step or answer, referencing prior conversation details (e.g., \"Since you rebooted the router\") if relevant. Keep the response natural and concise.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            # Clean response to remove any markdown\n",
        "            cleaned_response = re.sub(r'```(?:text)?\\n|\\n```', '', response.text).strip()\n",
        "            return cleaned_response\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error generating response: {e}\")\n",
        "            return \"Sorry, I encountered an issue. Please try again or provide more details.\"\n",
        "\n",
        "# Step 6: Simulate Troubleshooting Interaction\n",
        "chatbot = TroubleshootingChatbot()\n",
        "\n",
        "# Simulated conversation\n",
        "queries = [\n",
        "    \"My router isnâ€™t connecting to the internet. I just rebooted it.\",\n",
        "    \"The lights on the router are blinking amber now. What should I do?\"\n",
        "]\n",
        "\n",
        "for query in queries:\n",
        "    # Retrieve documents\n",
        "    retrieved_docs = chatbot.retrieve_documents(query)\n",
        "\n",
        "    # Generate response\n",
        "    response = chatbot.generate_response(query, retrieved_docs)\n",
        "\n",
        "    # Add to memory\n",
        "    chatbot.add_to_memory(query, response)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\nğŸ” User Query: {query}\")\n",
        "    print(\"\\nğŸ” Retrieved Documents:\")\n",
        "    for doc in retrieved_docs:\n",
        "        print(f\"- Doc {doc['doc_id']}: {doc['text']} (Score: {doc['score']:.4f})\")\n",
        "    print(\"\\nğŸ“ System Response:\")\n",
        "    print(response)\n",
        "\n",
        "# Step 7: Display Conversation History\n",
        "print(\"\\nğŸ“œ Conversation History:\")\n",
        "print(chatbot.get_context())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "k4ZgBID2r18C",
        "outputId": "83289387-5920-403f-dc00-76f651ce9253"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Collection 'Memory-Augmented' created.\n",
            "âœ… Documents upserted to Qdrant for dense retrieval.\n",
            "\n",
            "ğŸ” User Query: My router isnâ€™t connecting to the internet. I just rebooted it.\n",
            "\n",
            "ğŸ” Retrieved Documents:\n",
            "- Doc 1: If the router is not connecting to the internet after a reboot, check the Ethernet cable connection to the modem and ensure the modem is powered on. (Score: 7.3244)\n",
            "- Doc 4: Slow internet after rebooting the router may indicate interference. Change the Wi-Fi channel to 1, 6, or 11 in the router settings. (Score: 1.9415)\n",
            "- Doc 2: A common issue post-reboot is incorrect Wi-Fi settings. Verify the SSID and password in the routerâ€™s admin panel. (Score: 0.1264)\n",
            "\n",
            "ğŸ“ System Response:\n",
            "Since you rebooted the router and it's not connecting to the internet, first check that the Ethernet cable is securely connected to both the router and your modem.  Also, make sure your modem is powered on.\n",
            "\n",
            "ğŸ” User Query: The lights on the router are blinking amber now. What should I do?\n",
            "\n",
            "ğŸ” Retrieved Documents:\n",
            "- Doc 1: If the router is not connecting to the internet after a reboot, check the Ethernet cable connection to the modem and ensure the modem is powered on. (Score: 4.4877)\n",
            "- Doc 2: A common issue post-reboot is incorrect Wi-Fi settings. Verify the SSID and password in the routerâ€™s admin panel. (Score: 1.1245)\n",
            "- Doc 4: Slow internet after rebooting the router may indicate interference. Change the Wi-Fi channel to 1, 6, or 11 in the router settings. (Score: 1.0869)\n",
            "\n",
            "ğŸ“ System Response:\n",
            "Since you've rebooted and are seeing blinking amber lights, it's possible there's still an issue with the connection to your modem. Double-check that the Ethernet cable between the router and modem is firmly plugged in at both ends. If the cable is fine and the modem is powered on, check your router's manual to see what the amber lights indicate, as that varies between models.  If the modem has issues as well, contact your internet service provider.\n",
            "\n",
            "ğŸ“œ Conversation History:\n",
            "User: My router isnâ€™t connecting to the internet. I just rebooted it.\n",
            "System: Since you rebooted the router and it's not connecting to the internet, first check that the Ethernet cable is securely connected to both the router and your modem.  Also, make sure your modem is powered on.\n",
            "User: The lights on the router are blinking amber now. What should I do?\n",
            "System: Since you've rebooted and are seeing blinking amber lights, it's possible there's still an issue with the connection to your modem. Double-check that the Ethernet cable between the router and modem is firmly plugged in at both ends. If the cable is fine and the modem is powered on, check your router's manual to see what the amber lights indicate, as that varies between models.  If the modem has issues as well, contact your internet service provider.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Structured Data RAG**"
      ],
      "metadata": {
        "id": "n-qSN__ZsxYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "import sqlite3\n",
        "import re\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, Any\n",
        "\n",
        "# ENV variables\n",
        "GEMINI_API_KEY = \"AIzaSyBLWAaYmdhyTHlXAnULNGVxuxx21sq_HBg\"\n",
        "\n",
        "# Initialize Gemini\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Step 1: Set up SQLite database\n",
        "def setup_database():\n",
        "    \"\"\"Create and populate a sample sales database.\"\"\"\n",
        "    conn = sqlite3.connect(\":memory:\")  # In-memory database for demo\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Create sales table\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE sales (\n",
        "            sale_id INTEGER PRIMARY KEY,\n",
        "            sale_date DATE,\n",
        "            amount FLOAT,\n",
        "            product_category TEXT\n",
        "        )\n",
        "    \"\"\")\n",
        "\n",
        "    # Insert sample data (sales from Q4 2024 and earlier)\n",
        "    sample_sales = [\n",
        "        (\"2024-10-15\", 1500.50, \"Electronics\"),\n",
        "        (\"2024-11-01\", 800.25, \"Clothing\"),\n",
        "        (\"2024-12-10\", 1200.75, \"Electronics\"),\n",
        "        (\"2024-07-05\", 600.00, \"Books\"),\n",
        "        (\"2024-06-30\", 900.00, \"Clothing\"),\n",
        "    ]\n",
        "    cursor.executemany(\"INSERT INTO sales (sale_date, amount, product_category) VALUES (?, ?, ?)\", sample_sales)\n",
        "    conn.commit()\n",
        "    return conn\n",
        "\n",
        "# Step 2: Structured Data RAG Class\n",
        "class StructuredDataRAG:\n",
        "    def __init__(self, model_name: str = \"gemini-1.5-pro\", db_conn: sqlite3.Connection = None):\n",
        "        self.model = genai.GenerativeModel(model_name)\n",
        "        self.conn = db_conn\n",
        "        self.schema = \"\"\"\n",
        "        Table: sales\n",
        "        Columns:\n",
        "        - sale_id (INTEGER, PRIMARY KEY)\n",
        "        - sale_date (DATE, e.g., '2024-07-15')\n",
        "        - amount (FLOAT, sale amount in USD)\n",
        "        - product_category (TEXT, e.g., 'Electronics', 'Clothing')\n",
        "        \"\"\"\n",
        "\n",
        "    def generate_sql_query(self, query: str) -> str:\n",
        "        \"\"\"Generate an SQL query based on the natural language query.\"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        You are an expert SQL query generator. Given a natural language query and a database schema, generate a valid SQL query to retrieve the requested data. Return only the SQL query as plain text, without markdown, code blocks, or explanations.\n",
        "\n",
        "        Database Schema:\n",
        "        {self.schema}\n",
        "\n",
        "        Query: {query}\n",
        "\n",
        "        Example:\n",
        "        For \"total sales in 2024\", return: SELECT SUM(amount) FROM sales WHERE strftime('%Y', sale_date) = '2024'\n",
        "\n",
        "        Notes:\n",
        "        - For \"last quarter,\" assume the current date is {datetime.now().strftime('%Y-%m-%d')} and target the previous quarter (e.g., Q4 2024 for April 2025).\n",
        "        - Use strftime for date comparisons.\n",
        "\n",
        "        Generate the SQL query:\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            cleaned_response = re.sub(r'```(?:sql)?\\n|\\n```', '', response.text).strip()\n",
        "            return cleaned_response\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error generating SQL query: {e}\")\n",
        "            return \"SELECT 0 AS error\"  # Fallback query\n",
        "\n",
        "    def execute_query(self, sql_query: str) -> List[Dict]:\n",
        "        \"\"\"Execute the SQL query and return results as a list of dictionaries.\"\"\"\n",
        "        try:\n",
        "            cursor = self.conn.cursor()\n",
        "            cursor.execute(sql_query)\n",
        "            columns = [desc[0] for desc in cursor.description]\n",
        "            results = [dict(zip(columns, row)) for row in cursor.fetchall()]\n",
        "            # Handle null or empty results\n",
        "            if not results:\n",
        "                return [{\"total_sales\": 0.0}]\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error executing SQL query: {e}\")\n",
        "            return [{\"error\": \"Failed to execute query\"}]\n",
        "\n",
        "    def generate_response(self, query: str, data: List[Dict]) -> str:\n",
        "        \"\"\"Generate a natural language response based on retrieved data.\"\"\"\n",
        "        # Convert data to JSON string without f-string to avoid format specifier issues\n",
        "        data_str = json.dumps(data, indent=2)\n",
        "        # Build prompt as a regular string concatenation to avoid f-string issues\n",
        "        prompt = (\n",
        "            \"You are a data analyst. Given a natural language query and retrieved data from a database, \"\n",
        "            \"generate a concise, natural language response summarizing the results. \"\n",
        "            \"Return the response as plain text without markdown or code blocks.\\n\\n\"\n",
        "            \"Query: \" + query + \"\\n\\n\"\n",
        "            \"Retrieved Data: \" + data_str + \"\\n\\n\"\n",
        "            \"Example:\\n\"\n",
        "            \"Query: total sales in 2024\\n\"\n",
        "            \"Data: [{\\\"sum\\\": 5000.0}]\\n\"\n",
        "            \"Response: The total sales in 2024 were $5,000.\\n\\n\"\n",
        "            \"Generate the response:\"\n",
        "        )\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            cleaned_response = re.sub(r'```(?:text)?\\n|\\n```', '', response.text).strip()\n",
        "            # Handle null or zero results in the response\n",
        "            if any(\"total_sales\" in d and d[\"total_sales\"] == 0.0 for d in data):\n",
        "                return \"No sales were recorded for the last quarter.\"\n",
        "            return cleaned_response\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error generating response: {e}\")\n",
        "            return \"Sorry, I couldnâ€™t process the data. Please try again.\"\n",
        "\n",
        "# Step 3: Simulate Query\n",
        "query = \"total sales last quarter\"\n",
        "\n",
        "# Step 4: Initialize Database and RAG\n",
        "conn = setup_database()\n",
        "rag = StructuredDataRAG(db_conn=conn)\n",
        "\n",
        "# Step 5: Generate and Execute SQL Query\n",
        "sql_query = rag.generate_sql_query(query)\n",
        "print(\"\\nğŸ” Generated SQL Query:\")\n",
        "print(sql_query)\n",
        "\n",
        "data = rag.execute_query(sql_query)\n",
        "print(\"\\nğŸ” Retrieved Data:\")\n",
        "print(json.dumps(data, indent=2))\n",
        "\n",
        "# Step 6: Generate Response\n",
        "response = rag.generate_response(query, data)\n",
        "print(\"\\nğŸ“ Response:\")\n",
        "print(response)\n",
        "\n",
        "# Step 7: Clean up\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "U80oZNuLsg2l",
        "outputId": "d3eb78c6-21c2-4f5f-c85e-10e170a13796"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ” Generated SQL Query:\n",
            "SELECT SUM(amount) FROM sales WHERE strftime('%Y', sale_date) = '2024' AND strftime('%m', sale_date) BETWEEN '10' AND '12'\n",
            "\n",
            "ğŸ” Retrieved Data:\n",
            "[\n",
            "  {\n",
            "    \"SUM(amount)\": 3501.5\n",
            "  }\n",
            "]\n",
            "\n",
            "ğŸ“ Response:\n",
            "Total sales last quarter were $3,501.50.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Graph-Based RAG**"
      ],
      "metadata": {
        "id": "VXi-bhultyKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "import re\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# ENV variables\n",
        "GEMINI_API_KEY = \"AIzaSyBLWAaYmdhyTHlXAnULNGVxuxx21sq_HBg\"\n",
        "\n",
        "# Initialize Gemini\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Step 1: Mock Neo4j Knowledge Graph\n",
        "class MockNeo4j:\n",
        "    \"\"\"Simulate a Neo4j knowledge graph with movie data.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Sample graph data: nodes (Actor, Movie, Director) and relationships\n",
        "        self.graph_data = [\n",
        "            {\"actor\": \"Christian Bale\", \"movie\": \"The Dark Knight\", \"director\": \"Christopher Nolan\", \"year\": 2008},\n",
        "            {\"actor\": \"Heath Ledger\", \"movie\": \"The Dark Knight\", \"director\": \"Christopher Nolan\", \"year\": 2008},\n",
        "            {\"actor\": \"Leonardo DiCaprio\", \"movie\": \"Inception\", \"director\": \"Christopher Nolan\", \"year\": 2010},\n",
        "            {\"actor\": \"Joseph Gordon-Levitt\", \"movie\": \"Inception\", \"director\": \"Christopher Nolan\", \"year\": 2010},\n",
        "            {\"actor\": \"Cillian Murphy\", \"movie\": \"Oppenheimer\", \"director\": \"Christopher Nolan\", \"year\": 2023},\n",
        "            {\"actor\": \"Robert Downey Jr.\", \"movie\": \"Oppenheimer\", \"director\": \"Christopher Nolan\", \"year\": 2023},\n",
        "            {\"actor\": \"Keanu Reeves\", \"movie\": \"The Matrix\", \"director\": \"Wachowskis\", \"year\": 1999},\n",
        "        ]\n",
        "\n",
        "    def run_query(self, cypher_query: str) -> List[Dict]:\n",
        "        \"\"\"Simulate executing a Cypher query against the graph.\"\"\"\n",
        "        try:\n",
        "            results = []\n",
        "            # Parse common Cypher patterns\n",
        "            if \"MATCH (a:Actor)-[:ACTED_IN]->(m:Movie)<-[:DIRECTED]-(d:Director {name: 'Christopher Nolan'})\" in cypher_query:\n",
        "                for entry in self.graph_data:\n",
        "                    if entry[\"director\"] == \"Christopher Nolan\":\n",
        "                        result = {\"actor\": entry[\"actor\"], \"movie\": entry[\"movie\"]}\n",
        "                        if \"m.year\" in cypher_query:\n",
        "                            result[\"year\"] = entry[\"year\"]\n",
        "                        results.append(result)\n",
        "            elif \"MATCH (a:Actor)-[:ACTED_IN]->(m:Movie)\" in cypher_query and \"Christopher Nolan\" in cypher_query:\n",
        "                for entry in self.graph_data:\n",
        "                    if entry[\"director\"] == \"Christopher Nolan\":\n",
        "                        result = {\"actor\": entry[\"actor\"], \"movie\": entry[\"movie\"]}\n",
        "                        if \"m.year\" in cypher_query:\n",
        "                            result[\"year\"] = entry[\"year\"]\n",
        "                        results.append(result)\n",
        "            else:\n",
        "                results.append({\"error\": \"Unsupported query\"})\n",
        "            return results if results else [{\"error\": \"No results found\"}]\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error executing Cypher query: {e}\")\n",
        "            return [{\"error\": \"Failed to execute query\"}]\n",
        "\n",
        "# Step 2: Graph-Based RAG Class\n",
        "class GraphBasedRAG:\n",
        "    def __init__(self, model_name: str = \"gemini-1.5-pro\", graph_db: Any = None):\n",
        "        self.model = genai.GenerativeModel(model_name)\n",
        "        self.graph_db = graph_db\n",
        "        self.schema = \"\"\"\n",
        "        Knowledge Graph Schema:\n",
        "        Nodes:\n",
        "        - Actor (properties: name)\n",
        "        - Movie (properties: title, year)\n",
        "        - Director (properties: name)\n",
        "        Relationships:\n",
        "        - (:Actor)-[:ACTED_IN]->(:Movie)\n",
        "        - (:Director)-[:DIRECTED]->(:Movie)\n",
        "        Example:\n",
        "        (a:Actor {name: 'Christian Bale'})-[:ACTED_IN]->(m:Movie {title: 'The Dark Knight', year: 2008})<-[:DIRECTED]-(d:Director {name: 'Christopher Nolan'})\n",
        "        \"\"\"\n",
        "\n",
        "    def is_safe_cypher(self, query: str) -> bool:\n",
        "        \"\"\"Validate Cypher query for safety.\"\"\"\n",
        "        dangerous_keywords = [\"CREATE\", \"DELETE\", \"REMOVE\", \"SET\", \"MERGE\"]\n",
        "        return not any(keyword in query.upper() for keyword in dangerous_keywords)\n",
        "\n",
        "    def generate_cypher_query(self, query: str) -> str:\n",
        "        \"\"\"Generate a Cypher query based on the natural language query.\"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        You are an expert Cypher query generator for a Neo4j knowledge graph. Given a natural language query and a graph schema, generate a valid Cypher query to retrieve the requested data. Return only the Cypher query as plain text, without markdown, code blocks, or explanations.\n",
        "\n",
        "        Graph Schema:\n",
        "        {self.schema}\n",
        "\n",
        "        Query: {query}\n",
        "\n",
        "        Example:\n",
        "        For \"actors in Nolanâ€™s movies\", return:\n",
        "        MATCH (a:Actor)-[:ACTED_IN]->(m:Movie)<-[:DIRECTED]-(d:Director {{name: 'Christopher Nolan'}})\n",
        "        RETURN a.name AS actor, m.title AS movie\n",
        "\n",
        "        Generate the Cypher query:\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            cleaned_response = re.sub(r'```(?:cypher)?\\n|\\n```', '', response.text).strip()\n",
        "            if not self.is_safe_cypher(cleaned_response):\n",
        "                print(\"âš ï¸ Unsafe Cypher query detected\")\n",
        "                return \"MATCH () RETURN 'error' AS error\"\n",
        "            return cleaned_response\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error generating Cypher query: {e}\")\n",
        "            return \"MATCH () RETURN 'error' AS error\"\n",
        "\n",
        "    def execute_query(self, cypher_query: str) -> List[Dict]:\n",
        "        \"\"\"Execute the Cypher query against the graph database.\"\"\"\n",
        "        return self.graph_db.run_query(cypher_query)\n",
        "\n",
        "    def generate_response(self, query: str, data: List[Dict]) -> str:\n",
        "        \"\"\"Generate a natural language response based on retrieved graph data.\"\"\"\n",
        "        data_str = json.dumps(data, indent=2)\n",
        "        prompt = (\n",
        "            \"You are a data analyst. Given a natural language query and retrieved data from a knowledge graph, \"\n",
        "            \"generate a concise, natural language response summarizing the results. \"\n",
        "            \"Return the response as plain text without markdown or code blocks.\\n\\n\"\n",
        "            \"Query: \" + query + \"\\n\\n\"\n",
        "            \"Retrieved Data: \" + data_str + \"\\n\\n\"\n",
        "            \"Example:\\n\"\n",
        "            \"Query: actors in Nolanâ€™s movies\\n\"\n",
        "            \"Data: [{\\\"actor\\\": \\\"Christian Bale\\\", \\\"movie\\\": \\\"The Dark Knight\\\"}, {\\\"actor\\\": \\\"Leonardo DiCaprio\\\", \\\"movie\\\": \\\"Inception\\\"}]\\n\"\n",
        "            \"Response: Actors in Christopher Nolanâ€™s movies include Christian Bale (The Dark Knight) and Leonardo DiCaprio (Inception).\\n\\n\"\n",
        "            \"Generate the response:\"\n",
        "        )\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            cleaned_response = re.sub(r'```(?:text)?\\n|\\n```', '', response.text).strip()\n",
        "            if any(\"error\" in d for d in data):\n",
        "                return \"No relevant data found for the query.\"\n",
        "            return cleaned_response\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error generating response: {e}\")\n",
        "            return \"Sorry, I couldnâ€™t process the data. Please try again.\"\n",
        "\n",
        "    def verify_response(self, query: str, cypher_query: str, data: List[Dict], response: str) -> str:\n",
        "        \"\"\"Verify the accuracy of the generated response.\"\"\"\n",
        "        data_str = json.dumps(data, indent=2)\n",
        "        prompt = (\n",
        "            \"Verify if the response '\" + response + \"' accurately reflects the query '\" + query + \"', \"\n",
        "            \"Cypher query '\" + cypher_query + \"', and data \" + data_str + \".\\n\"\n",
        "            \"Return a plain text verdict without markdown or code blocks.\"\n",
        "        )\n",
        "        try:\n",
        "            verification = self.model.generate_content(prompt)\n",
        "            return re.sub(r'```(?:text)?\\n|\\n```', '', verification.text).strip()\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error verifying response: {e}\")\n",
        "            return \"Verification failed.\"\n",
        "\n",
        "# Step 3: Simulate Query\n",
        "query = \"actors in Nolanâ€™s movies\"\n",
        "\n",
        "# Step 4: Initialize Graph and RAG\n",
        "graph_db = MockNeo4j()\n",
        "rag = GraphBasedRAG(graph_db=graph_db)\n",
        "\n",
        "# Step 5: Generate and Execute Cypher Query\n",
        "cypher_query = rag.generate_cypher_query(query)\n",
        "print(\"\\nğŸ” Generated Cypher Query:\")\n",
        "print(cypher_query)\n",
        "\n",
        "data = rag.execute_query(cypher_query)\n",
        "print(\"\\nğŸ” Retrieved Data:\")\n",
        "print(json.dumps(data, indent=2))\n",
        "\n",
        "# Step 6: Generate Response\n",
        "response = rag.generate_response(query, data)\n",
        "print(\"\\nğŸ“ Response:\")\n",
        "print(response)\n",
        "\n",
        "# Step 7: Verify Response\n",
        "verification = rag.verify_response(query, cypher_query, data, response)\n",
        "print(\"\\nğŸ” Verification:\")\n",
        "print(verification)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "tBFMmN7rt0Sp",
        "outputId": "424488c5-f08e-4d0c-ab16-56906eadeadb"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ” Generated Cypher Query:\n",
            "MATCH (a:Actor)-[:ACTED_IN]->(m:Movie)<-[:DIRECTED]-(d:Director {name: 'Christopher Nolan'}) RETURN a.name\n",
            "\n",
            "ğŸ” Retrieved Data:\n",
            "[\n",
            "  {\n",
            "    \"actor\": \"Christian Bale\",\n",
            "    \"movie\": \"The Dark Knight\"\n",
            "  },\n",
            "  {\n",
            "    \"actor\": \"Heath Ledger\",\n",
            "    \"movie\": \"The Dark Knight\"\n",
            "  },\n",
            "  {\n",
            "    \"actor\": \"Leonardo DiCaprio\",\n",
            "    \"movie\": \"Inception\"\n",
            "  },\n",
            "  {\n",
            "    \"actor\": \"Joseph Gordon-Levitt\",\n",
            "    \"movie\": \"Inception\"\n",
            "  },\n",
            "  {\n",
            "    \"actor\": \"Cillian Murphy\",\n",
            "    \"movie\": \"Oppenheimer\"\n",
            "  },\n",
            "  {\n",
            "    \"actor\": \"Robert Downey Jr.\",\n",
            "    \"movie\": \"Oppenheimer\"\n",
            "  }\n",
            "]\n",
            "\n",
            "ğŸ“ Response:\n",
            "Actors in Christopher Nolan's movies include Christian Bale (The Dark Knight), Heath Ledger (The Dark Knight), Leonardo DiCaprio (Inception), Joseph Gordon-Levitt (Inception), Cillian Murphy (Oppenheimer), and Robert Downey Jr. (Oppenheimer).\n",
            "\n",
            "ğŸ” Verification:\n",
            "Yes, the response accurately reflects the query and the data.\n"
          ]
        }
      ]
    }
  ]
}